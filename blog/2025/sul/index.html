<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Supervised Learning | Zane Liao </title> <meta name="author" content="Zane Liao"> <meta name="description" content="Introduction to Machine Learning"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zane-liao.netlify.app//blog/2025/sul/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zane</span> Liao </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Supervised Learning</h1> <p class="post-meta"> Created on September 21, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> ¬† ¬∑ ¬† <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a> ¬† <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a> ¬† <a href="/blog/tag/code"> <i class="fa-solid fa-hashtag fa-sm"></i> code</a> ¬† ¬∑ ¬† <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction-to-supervised-learning">Introduction to Supervised Learning</h1> <ul> <li>Supervised learning uses labeled data for training.</li> <li>Therefore, when using supervised learning algorithms, you should pay attention to whether your dataset has been processed (i.e., cleaned, structured, and labeled).</li> <li>Machine learning algorithm tasks are divided into two major categories: regression and classification.</li> <li>Model types are generally categorized as discriminant and generative.</li> <li>Supervised learning algorithms generally include linear regression, as well as its derivatives, logistic regression, Naive Bayes, Support Vector Machine (SVM), Tree Type, and KNN.</li> <li>Linear regression and ridge regression generally have closed-form solutions. In other words, with a closed-form solution, the optimal solution can be obtained using formulas.</li> <li>Logistic regression, SVM, decision trees, and clustering algorithms do not have closed-form solutions; we need to use iteration to find the optimal solution.</li> <li>Models that can directly obtain theoretical optimal solutions using convex optimization tools include linear regression, ridge regression, Lasso, logistic regression, and SVM.</li> <li>Non-convex optimization models: decision trees, K-means, etc.</li> </ul> <h3 id="define">Define</h3> <p>[[1. Supervised Learning Formulations]]</p> <h3 id="regression-and-classification">Regression and Classification</h3> <p>Let‚Äôs first look at the definitions:</p> <ul> <li>Classification is learning a function ùëì: ùëã‚Üíùëå, where the output space ùëå is a discrete finite set</li> <li>Regression is learning a function ùëì: ùëã ‚Üí ùëå, where the output space ùëå ‚äÜ ùëÖ is a subset of the field of continuous real numbers Let:</li> <li>$\mathcal{X}$: input space (features)</li> <li>$\mathcal{Y}$: Output space (labels) <h5 id="classification">Classification</h5> </li> <li> <strong>Goal</strong>: Learn $f: \mathcal{X} \to \mathcal{Y}$, where \(\mathcal{Y} = \{1, 2, \dots, K\} \quad (K \ge 2)\) is a finite set (e.g., cat/dog/bird)</li> <li> <strong>Loss Function</strong>: The most common is the <strong>0-1 loss</strong>: \(L(f(x), y) = \begin{cases} 0 &amp; \text{if } f(x) = y \\ 1 &amp; \text{otherwise} \end{cases}\) or softmax + cross-entropy loss.</li> <li> <p><strong>Learning Method</strong>: Learn $P(y \mid x)$ or the decision boundary.</p> </li> <li>Classification Goal: $f^*(x) = \arg\max_{y \in \mathcal{Y}} P(y \mid x)$</li> </ul> <h5 id="regression">Regression</h5> <ul> <li> <strong>Goal</strong>: Learn $f: \mathcal{X} \to \mathbb{R}$, where</li> </ul> <p>\(\mathcal{Y} \subseteq \mathbb{R}\) is continuous, such as price, temperature, or rent.</p> <ul> <li> <strong>Loss Function</strong>: Typically, the mean squared error (MSE) or absolute error (MAE) is used: \(L(f(x), y) = (f(x) - y)^2\)</li> <li> <p><strong>Learning Method</strong>: Learn the expected value $\mathbb{E}[y \mid x]$ or the fitted function value.</p> </li> <li>Regression Goal: $f^*(x) = \mathbb{E}[y \mid x]$</li> </ul> <h5 id="visual-geometry">Visual Geometry</h5> <table> <thead> <tr> <th>Task</th> <th>Data Distribution Image</th> </tr> </thead> <tbody> <tr> <td>Classification</td> <td>Various points are distributed in feature space, and the model needs to draw lines/hyperplanes to separate them.</td> </tr> <tr> <td>Regression</td> <td>Points are distributed along a continuous curve, and the model needs to fit this curve.</td> </tr> </tbody> </table> <p>Source: Bishop, ‚ÄúPattern Recognition and Machine Learning‚Äù: Classification involves discrete labels, regression involves real-valued targets.</p> <p>Hastie, Tibshirani, Friedman, ‚ÄúThe Elements of Statistical Learning‚Äù: The goal of regression is to predict a continuous response; for classification, the response is categorical.</p> <hr> <h3 id="discriminant-and-generative-models">Discriminant and Generative Models</h3> <ul> <li> <p>Discriminant models are concerned with ‚Äúgiven an email, is it spam?‚Äù We learn the joint probability $P(y \mid x)$, which simultaneously models the input x and the output. The generation mechanism of y</p> </li> <li> <table> <tbody> <tr> <td>Generative models focus on the question ‚ÄúHow is a spam email written?‚Äù and directly learn the conditional probability distribution ùëÉ(ùë¶</td> <td>ùë•) or the decision boundary ùëì(ùë•)‚Üíùë¶ to achieve classification or regression tasks.</td> </tr> </tbody> </table> </li> </ul> <h5 id="linear-regression">Linear Regression</h5> <ul> <li> <p>Linear regression is the first supervised learning algorithm we learn. While it may seem simple at first glance, it‚Äôs actually quite complex due to its long history and widespread use in various fields. Many variants have emerged, making it quite complex. If you‚Äôre new to machine learning, it‚Äôs easy to be overwhelmed by this model. So, if you‚Äôre asked in an interview which model you‚Äôre most familiar with, avoid saying you‚Äôre familiar with linear regression. (ü§ì)</p> </li> <li> <p>Linear regression mainly includes the simplest Simple Linear Regression, the derived Multiple Linear Regression, Least Squares Linear Regression (OLS), and Generalized Linear Models (GLMs, for example, a variant is the Poisson Regression). Regression), linear regression with added L1 (Ridge) and L2 (Lasso) regularization terms (OLS + L2 = Ridge Regression), recursive least squares linear regression (RLS) for time series, nonlinear kernel regression (Kernel method), principal component regression (PCR), linear regression from a Bayesian perspective, etc.</p> </li> <li> <p>Linear regression has both discriminant and generative versions (Gaussian Joint Modeling)</p> </li> </ul> <h5 id="logistic-regression">Logistic Regression</h5> <ul> <li> <p>Logistic regression can be loosely considered linear regression for classification tasks.</p> </li> <li> <p>Logistic regression includes the simplest standardized logistic regression, logistic regression with added L1 and L2 terms (same as above), elastic net logistic regression (L1 + L2 = Elastic Net), softmax regression, kernel logistic regression (same as above), logistic regression from a Bayesian perspective (same as above), online logistic regression (SGD, AdaGrad), etc.</p> </li> <li> <p>Logistic regression also has both discriminant and generative versions (Gaussian Discriminant Analysis (GDA is a generative variant of logistic regression)</p> </li> </ul> <p><strong>Linear Regression and Logistic Regression are characterized by strong interpretability, fast training, and good accuracy, but are also susceptible to outliers</strong></p> <h5 id="naive-bayes">Naive Bayes</h5> <ul> <li> <p>Naive Bayes is a generative classification model. Why is it called ‚ÄúNaive‚Äù? Because all features are independent given the known class y.</p> </li> <li> <p>Naive Bayes is not a single model. It models $P(xi \mid y)$ based on a conditional distribution assumption. The conditional distribution is a large family of exponential distributions. The distribution to use depends on the input data type. For example, for text word frequency classification, we use a multinomial distribution (NB), while for image feature classification, we use a Gaussian NB or Bernoulli NB.</p> </li> <li> <p>Naive Bayes is characterized by fast training, the assumption of conditional independence, and robustness when data is scarce.</p> </li> </ul> <h5 id="support-vector-machine-svm">Support Vector Machine (SVM)</h5> <ul> <li> <p>Support Vector Machine (SVM) is a linear discriminant model that finds a hyperplane in feature space that maximizes the geometric margin to optimally distinguish between two classes of data.</p> </li> <li>Margin: The distance from the hyperplane to the closest sample point.</li> <li>Support Vectors: The sample points $y^{(i)} (w^T x^{(i)} + b) = 1$ that are closest to the hyperplane and satisfy the bounds.</li> <li> <p>SVM is essentially a maximum margin classifier.</p> </li> <li> <p>Real data is often linearly inseparable or noisy, so we cannot use a hard margin. Instead, we introduce a slack variable $\xi_i \ge 0$ (allowing some samples to cross the bounds).</p> </li> <li>The original SVM problem is a convex optimization problem, which is more suitable for computing kernel functions.</li> <li>SVM can be extended to nonlinear spaces by using the kernel function K(x,x‚Ä≤) instead of the inner product.</li> </ul> <h6 id="kernel-method">Kernel Method</h6> <ul> <li> <p>Kernel methods implicitly map the original input to a high-dimensional feature space by defining a kernel function K(x,x‚Ä≤), thereby constructing a linear model in this space without explicitly performing feature mapping.</p> </li> <li> <p>Why do we need kernel methods? Many data are linearly inseparable in the original space, but may be linearly separable in a higher-dimensional space. However, in a higher-dimensional space, the computational cost is very high or even impossible. We introduce an implicit mapping (calculating the inner product $\phi(x)^T \phi(x‚Äô)$) and avoid directly constructing a high-dimensional space (directly using $K(x, x‚Äô)$).</p> </li> <li> <p>Mathematical definition: A function $K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is a kernel function if and only if there exists a <strong>Hilbert space</strong> $\mathcal{H}$ and a mapping $\phi: \mathcal{X} \rightarrow \mathcal{H}$ such that: \(K(x, x') = \langle \phi(x), \phi(x') \rangle_{\mathcal{H}}\)</p> </li> <li> <p>Kernel functions typically come in many forms, including linear kernels, polynomial kernels, Gaussian kernels (RBF kernels), and sigmoid kernels.</p> </li> <li>An example is the original dual form of SVM: $f(x) = \text{sign}\left( \sum_{i=1}^m \alpha_i y^{(i)} \langle x^{(i)}, x \rangle + b \right)$</li> <li> <p>After replacing the inner product with a kernel function: $f(x) = \text{sign}\left( \sum_{i=1}^m \alpha_i y^{(i)} K(x^{(i)}, x) + b \right)$</p> </li> <li> <p>Common types of kernel methods include SVM with kernel, Kernel PCA, Kernel Ridge Regression, and Kernel K-means.</p> </li> <li>Given a set of samples, the kernel matrix (Gram Matrix) $x^{(1)}, \dots, x^{(n)}$, definition: $K_{ij} = K(x^{(i)}, x^{(j)})$ then $K \in \mathbb{R}^{n \times n}$ is a positive semidefinite matrix and must satisfy the following: <ol> <li>For any $\alpha \in \mathbb{R}^n$, $\alpha^T K \alpha \ge 0$</li> <li>This is the core of Mercer‚Äôs theorem: the kernel function must be positive definite.</li> </ol> </li> </ul> <h5 id="covariance-matrix">Covariance Matrix</h5> <ul> <li> <p><strong>Covariance Matrix</strong> describes how multiple variables change together: the degree of correlation between each pair of variables is described by covariance, and the ‚Äújoint correlation‚Äù of the entire set of variables constitutes the covariance matrix</p> </li> <li> <p><strong>Math Define</strong>: Suppose you have $m$ samples, each sample is an $n$-dimensional vector (that is, you have $n$ variables): \(X = \begin{bmatrix} \text{---} (x^{(1)})^T \text{---} \\ \text{---} (x^{(2)})^T \text{---} \\ \vdots \\ \text{---} (x^{(m)})^T \text{---}\end{bmatrix} \in \mathbb{R}^{m \times n}\) Sample mean vector: \(\mu = \frac{1}{m} \sum_{i=1}^m x^{(i)} \in \mathbb{R}^n\) <strong>Covariance matrix</strong> $\Sigma \in \mathbb{R}^{n \times n}$ is defined as: \(\Sigma = \frac{1}{m} \sum_{i=1}^m (x^{(i)} - \mu)(x^{(i)} - \mu)^T = \mathbb{E}\left[(x - \mu)(x - \mu)^T\right]\) Can also be written in matrix form (sample matrix $X$, one sample per row): \(\Sigma = \frac{1}{m} (X - \mu)^T (X - \mu)\)</p> </li> <li>We know that $\Sigma_{ij}$ represents the covariance between the $i$th variable and the $j$th variable: \(\Sigma_{ij} = \text{Cov}(x_i, x_j) = \mathbb{E}[(x_i - \mu_i)(x_j - \mu_j)]\) So:</li> <li>$\Sigma_{ii} = \text{Var}(x_i)$: variance of variable $x_i$</li> <li>$\Sigma_{ij} &gt; 0$: positive correlation</li> <li>$\Sigma_{ij} &lt; 0$: negative correlation</li> <li>$\Sigma_{ij} = 0$: uncorrelated (but not necessarily independent)</li> </ul> <h6 id="visual-geometry-1">Visual Geometry</h6> <ul> <li>The covariance matrix describes the ‚Äúscatter‚Äù of data points in all directions.</li> <li> <p>It can be thought of as the <strong>shape outline</strong> of the data. * If you plot a 2D data point cloud, the <strong>eigenvector</strong> of the covariance matrix gives the principal direction, and the <strong>eigenvalue</strong> gives the variance (length) in that direction ‚Üí Principal Component Analysis (PCA) selects the principal eigenvector of the covariance matrix as the projection direction.</p> </li> <li>The Covariance Matrix generally satisfies <ol> <li>$\Sigma = \Sigma^{T}$ (symmetry)</li> <li>$v^T \Sigma v \ge 0 \, \forall v$ (positive semidefinite)</li> <li>$\lambda_i \ge 0$ (all eigenvalues ‚Äã‚Äãare non-negative)</li> <li>Positive definite (if all samples are independent)</li> <li>Diagonalizable (orthogonal eigenvectors are diagonalized), etc.</li> </ol> </li> <li>Application Scenarios: PCA GDA Gaussian Process Multiple Variable Normal Distribution Model Whitening et al.</li> </ul> <p>For Example:</p> <ul> <li>Suppose we have 2 variables:</li> </ul> <p>| x‚ÇÅ | x‚ÇÇ | | ‚Äî | ‚Äî | | 1 | 2 | | 2 | 4 | | 3 | 6 | | 4 | 8 | x‚ÇÇ = 2 * x‚ÇÅ =&gt; linear correlation</p> <p>Compute the Covariance Matrix:</p> <p>\(\Sigma = \begin{bmatrix} \text{Var}(x_1) &amp; \text{Cov}(x_1, x_2) \\ \text{Cov}(x_2, x_1) &amp; \text{Var}(x_2) \end{bmatrix} = \begin{bmatrix} 1.67 &amp; 3.33 \\ 3.33 &amp; 6.67 \end{bmatrix}\)</p> <h6 id="why-can-the-covariance-matrix-be-used-directly-in-a-movie-recommendation-system-based-on-collaborative-filtering">Why can the Covariance Matrix be used directly in a movie recommendation system based on collaborative filtering?</h6> <p>Basic Idea: In a recommendation system, if the covariance between the ratings of two movies is high, it means that if a user likes movie A, they are also likely to like movie B (or that their ratings have the same trend). These movies are moving in the same direction in the ‚Äúuser rating space,‚Äù so we can use the covariance to determine the similarity between the movies (covariance = similarity).</p> <h5 id="tree-type">Tree Type</h5> <ul> <li>Tree algorithms are widely used in various fields, from regression and classification to ranking and anomaly detection.</li> <li>There are many tree-type algorithms, which can be roughly divided into two categories: simple trees and ensemble trees.</li> <li>The simplest tree is the decision tree, with variants like CART (Classification and Regression Tree).</li> <li>Decision trees can be used for regression tasks.</li> <li>Ensemble trees include Random Forest (an ensemble of multiple CART trees) and Boosted Trees. Boosted Trees include Gradient Boosting Decision Trees (GBDT), XGBoost, LightGBM, CatBoost, AdaBoost, etc.</li> </ul> <h5 id="knn">KNN</h5> <ul> <li>Simply put, the KNN (nearest neighbor) algorithm is a parameter-free, distance-based classification or regression method. It‚Äôs a type of lazy learning algorithm and is commonly used in classification and regression tasks. It‚Äôs suitable for small sample sizes and nonlinear problems, but it‚Äôs inefficient in large-scale data or high-dimensional spaces and requires optimization.</li> </ul> <h5 id="kd-tree">KD-Tree</h5> <ul> <li>Again, briefly, the KD-Tree is not an algorithm, but rather a special binary tree data structure.</li> <li>A strict definition: <strong>KD-Tree is a data structure that supports fast multidimensional space retrieval</strong>. It‚Äôs used for nearest neighbor searches and range searches in $\mathbb{R}^k$.</li> <li>The KD-Tree is primarily used in the KNN algorithm to quickly find the nearest neighbor.</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/policy/">Policy Gradient</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/deep/">Deep Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/bs/">Black-Scholes Model</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Zane Liao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>