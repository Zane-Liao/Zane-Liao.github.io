<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://zane-liao.netlify.app//feed.xml" rel="self" type="application/atom+xml"/><link href="https://zane-liao.netlify.app//" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-13T04:39:51+00:00</updated><id>https://zane-liao.netlify.app//feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Policy Gradient</title><link href="https://zane-liao.netlify.app//blog/2025/policy/" rel="alternate" type="text/html" title="Policy Gradient"/><published>2025-10-22T19:20:00+00:00</published><updated>2025-10-22T19:20:00+00:00</updated><id>https://zane-liao.netlify.app//blog/2025/policy</id><content type="html" xml:base="https://zane-liao.netlify.app//blog/2025/policy/"><![CDATA[<ul> <li> <p><strong>Derivation of Policy Gradient</strong></p> </li> <li> <p>The goal of reinforcement learning is to maximize reward $r$. The policy $\pi$ is defined as selecting the optimal action to maximize $r$.</p> </li> <li> <p>Policy Gradient is a method that directly adjusts the Action Method (Policy) to maximize reward.</p> </li> <li> <table> <tbody> <tr> <td>In Policy Gradient, our goal is to optimize the policy $\pi(a</td> <td>s, \theta)$ to perform actions with higher rewards.</td> </tr> </tbody> </table> </li> </ul> <p><strong>We aim to maximize an objective function $J(\theta)$, which is typically the expected cumulative reward of the agent after executing policy $\pi$.</strong></p> <p>$$ J(\theta) = \mathbb{E}<em>{\tau \sim \pi</em>\theta} \left[ \sum_{t=0}^\infty \gamma^t r_t \right]</p> <p>$$ <strong>Our Policy Gradient theorem is</strong></p> <p>$$ \nabla J(\theta) \propto \sum_{s} \mu(s) \sum_{a} q_\pi(s,a)\, \nabla_\theta \pi(a|s,\theta)</p> <p>$$</p> <ul> <li> <p>$\nabla J(\theta)$ is the gradient of policy performance with respect to parameters.</p> </li> <li> <p>$\propto$ is proportional to, in simple terms, it‚Äôs either directly proportional or inversely proportional; variable x increases or decreases along with x.</p> </li> <li> <p>$\sum_{s} \mu(s)$ is the robust distribution (state visitation frequency) of state $s$ under policy $\pi$, representing ‚Äúhow frequently the agent visits state $s‚Äù in the long run. Sometimes it‚Äôs $\rho^\pi(s)$, but it‚Äôs the same; it‚Äôs a <strong>sampling probability</strong> used to weight the states.</p> </li> <li> <table> <tbody> <tr> <td>$\sum_a q_\pi(s,a)\, \nabla_\theta \pi(a</td> <td>s,\theta)$ is a weighted average of all possible actions $a$ for each state $s$.</td> </tr> </tbody> </table> </li> <li> <p>$q_\pi(s,a)$ in state s <strong>Expected reward (Q-value)</strong> after performing action a</p> </li> <li> <table> <tbody> <tr> <td>$\nabla_\theta \pi(a</td> <td>s,\theta)$ is the gradient of the policy function with respect to the parameters, representing ‚Äúhow to fine-tune the parameters to increase the probability of the action‚Äù.</td> </tr> </tbody> </table> </li> </ul> <p><strong>Proof of our Policy Gradient Theorem (episodic case, from Sutton textbook)</strong></p> <ul> <li>The state-value function can be represented in the form of an action-value function.</li> </ul> <p>$$ \begin{align*}</p> <table> <tbody> <tr> <td>\nabla v_{\pi}(s) &amp;= \nabla \left[\sum_{a}\pi(a</td> <td>s)q_{\pi}(s,a)\right],\</td> </tr> </tbody> </table> <p>\text{for all s $\in$ S} \</p> <table> <tbody> <tr> <td>&amp;= \sum_{a}\left[\nabla \pi(a</td> <td>s)q_{\pi}(s,a)+\pi(a</td> <td>s)\nabla q_{\pi}(s,a)\right]</td> </tr> </tbody> </table> <p>\text{(rule of calculus)} \</p> <table> <tbody> <tr> <td>&amp;= \sum_{a}\left[\nabla \pi(a</td> <td>s)q_{\pi}(s,a)+\pi(a</td> <td>s)\nabla \sum_{s‚Äô, r} q(s‚Äô,r</td> <td>s,a)(r+v_{\pi}(s‚Äô))\right] \</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>&amp;= \sum_{a}\left[\nabla \pi(a</td> <td>s)q_{\pi}(s,a)+\pi(a</td> <td>s) \sum_{s‚Äô} q(s‚Äô</td> <td>s,a)\nabla v_{\pi}(s‚Äô)\right] \</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>&amp;= \sum_{a}\left[\nabla \pi(a</td> <td>s)q_{\pi}(s,a)+\pi(a</td> <td>s) \sum_{s‚Äô} q(s‚Äô</td> <td>s,a)</td> </tr> <tr> <td>\sum_{a‚Äô}\nabla \pi(a‚Äô</td> <td>s‚Äô)q_{\pi}(s‚Äô,a‚Äô)+\pi(a‚Äô</td> <td>s‚Äô) \sum_{s‚Äô‚Äô} q(s‚Äô‚Äô</td> <td>s‚Äô,a‚Äô)\nabla v_{\pi}(s‚Äô‚Äô) \right] \</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>&amp;= \sum_{x \in S} \sum^{\infty}<em>{k=0} Pr(s \to x,k,\pi) \sum</em>{a} \nabla_{\pi}(a</td> <td>x)q_{\pi}(x,a) \</td> </tr> </tbody> </table> <p>\end{align*}</p> <p>$$</p> <ul> <li>The probability that we transition from state s to state x</li> </ul> <p>$$ \begin{align*}</p> <p>\nabla J(\theta) &amp;= \nabla v_{\pi}(s_{0}) \</p> <table> <tbody> <tr> <td>&amp;= \sum_{s} \left(\sum^{\infty}<em>{k=0} Pr(s</em>{0} \to s,k,\pi)\right) \sum_{a} \nabla_{\pi}(a</td> <td>s)q_{\pi}(s,a) \</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>&amp;= \sum_{s} \eta(s) \sum_{a} \nabla_{\pi}(a</td> <td>s)q_{\pi}(s,a) \</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>&amp;= \sum_{s‚Äô} \eta(s‚Äô) \sum_{s} \frac{\eta(s)}{\sum_{s‚Äô}\eta(s‚Äô)} \sum_{a} \nabla_{\pi}(a</td> <td>s)q_{\pi}(s,a) \</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>&amp;= \sum_{s‚Äô} \eta(s‚Äô) \sum_{s} \mu(s) \sum_{a} \nabla_{\pi}(a</td> <td>s)q_{\pi}(s,a) \</td> </tr> <tr> <td>&amp;\propto \sum_{s} \mu(s) \sum_{a} \nabla_{\pi}(a</td> <td>s)q_{\pi}(s,a)</td> </tr> </tbody> </table> <p>\ \text{End.} \</p> <p>\end{align*}</p> <p>$$ Policy gradient methods generally include</p> <ul> <li> <p>Policy gradient classification includes On-policy, Off-policy, and Iteration</p> </li> <li> <p>On-policy must be sampled from the current policy $\pi_{\theta}$</p> </li> <li> <p>Off-policy allows sampling from other policies $\mu$ to update the policy $\pi$</p> </li> <li> <table> <tbody> <tr> <td>In $\nabla_\theta J(\theta) = \mathbb{E}<em>{\tau \sim \mu}\left[\frac{\pi</em>\theta(a</td> <td>s)}{\mu(a</td> <td>s)} \nabla_\theta \log \pi_\theta(a</td> <td>s) \, G_t \right]$, $\frac{\pi_\theta}{\mu}$ belongs to importance sampling. (IS)</td> </tr> </tbody> </table> </li> <li> <p>Importance Sampling is a general method in probability theory used to calculate the expectation of a target distribution $p$, but the samples come from another distribution $q$. In $\mathbb{E}<em>{x \sim p}[f(x)] = \mathbb{E}</em>{x \sim q}\left[\frac{p(x)}{q(x)} f(x)\right]$, $\rho(x) = \frac{p(x)}{q(x)}$ are the IS weights.</p> </li> <li> <p>Iteration refers to both the concept of ‚Äúpolicy iteration‚Äù (evaluation + improvement) and the <strong>mini-batch multi-round updates</strong> in actual implementation.</p> </li> <li> <p>The most basic <strong>REINFORCE</strong> method uses Montto Carlo sampling estimation.</p> </li> <li> <p><strong>REINFORCE with Baseline</strong> method introduces a <strong>baseline</strong> to reduce variance.</p> </li> <li> <p>Actor-Critic method: the policy function is used to select actions, and the value function is used to evaluate the quality of the actions.</p> </li> <li> <p>Proximal Policy Optimization (PPO) Algorithms</p> </li> <li> <p>Direct Preference Optimization (DPO) Algorithm</p> </li> <li> <p>Group Relative Policy Optimization (GRPO) Algorithm (DeepSeek R1), etc.</p> </li> <li> <p><strong>Eligibility Traces</strong> are a temporal credit allocation mechanism (TD(Œª), Actor-Critic(Œª)), essentially making gradients propagate more smoothly over time. In Deep RL, we use GAE (Generalized Advantage Estimation) instead.</p> </li> <li>When implementing these algorithms, we distinguish between <strong>episodic</strong> (with an end, like the end of a game) and <strong>continuing</strong> (without an end, infinite). The focus is on PPO, DPO, and GRPO.</li> </ul> <h5 id="reinforce">REINFORCE</h5> <p>\(\nabla J(\theta) \propto \sum_{s} \mu(s) \sum_{a} q_\pi(s,a)\, \nabla_\theta \pi(a|s,\theta) = \mathbb{E} \left[ \sum_a q_{\pi}(S_t, a)\nabla_{\pi}(a|S_t, \theta) \right] \tag{13.6}\)</p> <ul> <li>Gradient-Ascent Algorithm Update-step: \(\theta_{t+1} := \theta_t + \alpha \sum_a \hat{q}(S_t,a,w)\nabla_{\pi}(a|S_t,\theta)\)</li> <li>Continuing from 13.6, we deduce sampling (sampling) $$ \begin{aligned}</li> </ul> <table> <tbody> <tr> <td>J(\theta) &amp;= \mathbb{E_{\pi}} \left[ \sum_a \pi(a</td> <td>S_t, \theta) q_{\pi}(S_t, a)\frac{\nabla_{\pi}(a</td> <td>S_t, \theta)}{\pi(a</td> <td>S_t, \theta)} \right] \</td> </tr> <tr> <td>&amp;= \mathbb{E_{\pi}} \left[ q_{\pi}(S_t, A_t)\frac{\nabla_{\pi}(A_t</td> <td>S_t, \theta)}{\pi(A_t</td> <td>S_t, \theta)} \right] \text{(replacing a by the sample)}\</td> <td>¬†</td> </tr> <tr> <td>&amp;= \mathbb{E_{\pi}} \left[G_t \frac{\nabla_{\pi}(A_t</td> <td>S_t, \theta)}{\pi(A_t</td> <td>S_t, \theta)} \right] \text{[$\mathbb{E}_{\pi}[G_t</td> <td>S_t,A_t]=q_{\pi}(S_t,A_t)$]}</td> </tr> </tbody> </table> <p>\end{aligned} $$</p> <ul> <li>REINFORCE Update: \(\theta_{t+1} := \theta_t + \alpha G_t \frac{\nabla_{\pi}(A_t|S_t, \theta)}{\pi(A_t|S_t, \theta)}\)</li> </ul> <h5 id="reinforce-with-baseline">REINFORCE with Baseline</h5> <p>\(\nabla J(\theta) \propto \sum_{s} \mu(s) \left(\sum_{a} q_\pi(s,a)-b(s)\right) \nabla_\theta \pi(a|s,\theta)\)</p> <ul> <li>REINFORCE with Baseline Update: \(\theta_{t+1} := \theta_t + \alpha \big(G_t - b(S_t)\big) \frac{\nabla_{\pi}(A_t|S_t, \theta)}{\pi(A_t|S_t, \theta)}\)</li> </ul> <h5 id="actor-critic">Actor-Critic</h5> <ul> <li>Let‚Äôs look directly at the derivation in sutton, Update: $$ \begin{aligned} \theta_{t+1} &amp;:= \theta_t + \alpha \left( G_{t:t+1} - \hat{v}(S_t, w) \right) \frac{\nabla_{\pi}(A_t|S_t, \theta)}{\pi(A_t|S_t, \theta)} <br/> &amp;= \theta_t + \alpha \left( G_{t:t+1} - \gamma \hat{v}(S_{t+1}, w) - \hat{v}(S_t, w) \right) \frac{\nabla_{\pi}(A_t|S_t, \theta)}{\pi(A_t|S_t, \theta)} \</li> </ul> <table> <tbody> <tr> <td>&amp;= \theta_t + \alpha \delta \frac{\nabla_{\pi}(A_t</td> <td>S_t, \theta)}{\pi(A_t</td> <td>S_t, \theta)}</td> </tr> </tbody> </table> <p>\end{aligned}</p> <p>$$</p> <h5 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</h5> <ul> <li> <p>Since algorithms like PPO differ significantly from those described in papers, derivation is omitted; the focus is on implementation and details.</p> </li> <li> <p>Key parts</p> </li> <li> <table> <tbody> <tr> <td>Policy ratio: $r_t(\theta)=\dfrac{\pi_\theta(a_t</td> <td>s_t)}{\pi_{\text{old}}(a_t</td> <td>s_t)}$</td> </tr> </tbody> </table> </li> <li>PPO-Clip surrogate $L^{\text{CLIP}}(\theta)=\mathbb{E}_t\Big[\min\big(r_t(\theta)\hat A_t,\; \operatorname{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat A_t\big)\Big]$</li> <li>GAE (Advantage Estimation) $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t),\quad \hat A_t = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}$</li> <li>Merge(total loss) value + entropy $L(\theta) = -L^{\text{CLIP}}(\theta) + c_1 \, \mathbb{E}<em>t\big[(V</em>\theta(s_t)-R_t)^2\big] - c_2 \, \mathbb{E}_t\big[ S<a href="s_t">\pi_\theta</a>\big]$</li> </ul> <hr/> <p>Algorithm 1: Proximal Policy Optimization (clipped surrogate) <em>**</em></p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Initialize</span> <span class="err">Œ∏</span> <span class="err">‚Üê</span> <span class="err">Œ∏</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">...</span> <span class="k">do</span>
    <span class="mi">1</span><span class="p">.</span> <span class="n">Collect</span> <span class="n">set</span> <span class="n">of</span> <span class="n">transitions</span> <span class="n">D</span> <span class="o">=</span> <span class="p">{</span> <span class="p">(</span><span class="n">s_t</span><span class="p">,</span> <span class="n">a_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">done_t</span><span class="p">,</span> <span class="n">log</span><span class="err">œÄ</span><span class="n">_old_t</span><span class="p">,</span> <span class="n">V_old_t</span><span class="p">)</span> <span class="p">}</span>
       <span class="n">by</span> <span class="n">running</span> <span class="n">policy</span> <span class="err">œÄ</span><span class="n">_</span><span class="p">{</span><span class="err">Œ∏</span><span class="p">}</span> <span class="n">in</span> <span class="n">the</span> <span class="n">environment</span> <span class="k">for</span> <span class="n">T</span> <span class="err">√ó</span> <span class="n">N</span> <span class="n">steps</span><span class="o">:</span>
           <span class="k">for</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">..</span> <span class="n">T</span><span class="o">-</span><span class="mi">1</span> <span class="p">(</span><span class="n">parallel</span> <span class="n">over</span> <span class="n">N</span> <span class="n">envs</span><span class="p">)</span><span class="o">:</span>
               <span class="n">a_t</span> <span class="o">~</span> <span class="err">œÄ</span><span class="n">_</span><span class="p">{</span><span class="err">Œ∏</span><span class="p">}(</span><span class="err">¬∑</span><span class="o">|</span><span class="n">s_t</span><span class="p">)</span>
               <span class="n">record</span> <span class="n">log</span><span class="err">œÄ</span><span class="n">_old_t</span> <span class="err">‚Üê</span> <span class="n">log</span> <span class="err">œÄ</span><span class="n">_</span><span class="p">{</span><span class="err">Œ∏</span><span class="p">}(</span><span class="n">a_t</span><span class="o">|</span><span class="n">s_t</span><span class="p">)</span>
               <span class="n">execute</span> <span class="n">a_t</span> <span class="err">‚Üí</span> <span class="n">observe</span> <span class="n">reward</span> <span class="n">r_t</span> <span class="n">and</span> <span class="n">next</span> <span class="n">state</span> <span class="n">s_</span><span class="p">{</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">},</span> <span class="n">done_t</span>
               <span class="n">record</span> <span class="n">V_old_t</span> <span class="err">‚Üê</span> <span class="n">V_</span><span class="p">{</span><span class="err">Œ∏</span><span class="p">}(</span><span class="n">s_t</span><span class="p">)</span>   <span class="err">#</span> <span class="n">critic</span><span class="err">'</span><span class="n">s</span> <span class="n">prediction</span> <span class="n">at</span> <span class="n">sample</span> <span class="n">time</span>
    <span class="mi">2</span><span class="p">.</span> <span class="n">Compute</span> <span class="n">advantages</span> <span class="err">ÀÜ</span><span class="n">A_t</span> <span class="n">and</span> <span class="n">returns</span> <span class="n">R_t</span> <span class="n">via</span> <span class="n">GAE</span><span class="o">:</span>
       <span class="cp"># Œ¥_t = r_t + Œ≥ V(s_{t+1}) (1 - done_{t+1}) - V(s_t)
</span>       <span class="cp"># ÀÜA_t = Œ¥_t + Œ≥ Œª Œ¥_{t+1} + Œ≥^2 Œª^2 Œ¥_{t+2} + ...
</span>       <span class="n">Compute</span> <span class="err">ÀÜ</span><span class="n">A_t</span> <span class="k">for</span> <span class="n">all</span> <span class="n">t</span> <span class="n">by</span> <span class="n">reversed</span> <span class="n">recursion</span> <span class="p">(</span><span class="n">see</span> <span class="n">GAE</span> <span class="n">formula</span><span class="p">).</span>
       <span class="n">Set</span> <span class="n">R_t</span> <span class="err">‚Üê</span> <span class="err">ÀÜ</span><span class="n">A_t</span> <span class="o">+</span> <span class="n">V_old_t</span>
       <span class="n">Optionally</span> <span class="n">normalize</span> <span class="n">advantages</span><span class="o">:</span> <span class="err">ÀÜ</span><span class="n">A_t</span> <span class="err">‚Üê</span> <span class="p">(</span><span class="err">ÀÜ</span><span class="n">A_t</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span><span class="o">/</span><span class="n">std</span>

    <span class="mi">3</span><span class="p">.</span> <span class="n">Let</span> <span class="err">Œ∏</span><span class="n">_old</span> <span class="err">‚Üê</span> <span class="err">Œ∏</span>   <span class="err">#</span> <span class="n">freeze</span> <span class="n">old</span> <span class="n">policy</span> <span class="k">for</span> <span class="n">ratio</span> <span class="n">computation</span>

    <span class="mi">4</span><span class="p">.</span> <span class="n">For</span> <span class="n">epoch</span> <span class="o">=</span> <span class="mi">1</span> <span class="p">..</span> <span class="n">K</span> <span class="k">do</span>
           <span class="n">Shuffle</span> <span class="n">indices</span> <span class="n">of</span> <span class="n">D</span>
           <span class="n">Partition</span> <span class="n">D</span> <span class="n">into</span> <span class="n">minibatches</span> <span class="n">of</span> <span class="n">size</span> <span class="n">M</span>
           <span class="n">For</span> <span class="n">each</span> <span class="n">minibatch</span> <span class="n">B</span> <span class="k">do</span>
               <span class="o">-</span> <span class="n">Evaluate</span> <span class="n">new</span> <span class="n">log</span> <span class="n">probabilities</span> <span class="n">and</span> <span class="n">values</span><span class="o">:</span>
                   <span class="n">log</span><span class="err">œÄ</span><span class="n">_new_t</span> <span class="err">‚Üê</span> <span class="n">log</span> <span class="err">œÄ</span><span class="n">_</span><span class="p">{</span><span class="err">Œ∏</span><span class="p">}(</span><span class="n">a_t</span><span class="o">|</span><span class="n">s_t</span><span class="p">)</span>   <span class="k">for</span> <span class="n">t</span> <span class="err">‚àà</span> <span class="n">B</span>
                   <span class="n">V_new_t</span>    <span class="err">‚Üê</span> <span class="n">V_</span><span class="p">{</span><span class="err">Œ∏</span><span class="p">}(</span><span class="n">s_t</span><span class="p">)</span>
                   <span class="n">S_t</span>        <span class="err">‚Üê</span> <span class="n">entropy</span> <span class="n">of</span> <span class="err">œÄ</span><span class="n">_</span><span class="p">{</span><span class="err">Œ∏</span><span class="p">}(</span><span class="err">¬∑</span><span class="o">|</span><span class="n">s_t</span><span class="p">)</span>

               <span class="o">-</span> <span class="n">Compute</span> <span class="n">probability</span> <span class="n">ratios</span><span class="o">:</span>
                   <span class="n">r_t</span> <span class="err">‚Üê</span> <span class="n">exp</span><span class="p">(</span> <span class="n">log</span><span class="err">œÄ</span><span class="n">_new_t</span> <span class="err">‚àí</span> <span class="n">log</span><span class="err">œÄ</span><span class="n">_old_t</span> <span class="p">)</span>

               <span class="o">-</span> <span class="n">Clipped</span> <span class="n">surrogate</span> <span class="n">objective</span> <span class="p">(</span><span class="n">paper</span> <span class="n">eq</span><span class="p">.</span><span class="mi">7</span><span class="p">)</span><span class="o">:</span>
                   <span class="n">L</span><span class="o">^</span><span class="p">{</span><span class="n">CLIP</span><span class="p">}</span><span class="n">_t</span><span class="p">(</span><span class="err">Œ∏</span><span class="p">)</span> <span class="o">=</span> <span class="n">min</span><span class="p">(</span> <span class="n">r_t</span> <span class="o">*</span> <span class="err">ÀÜ</span><span class="n">A_t</span><span class="p">,</span>  <span class="n">clip</span><span class="p">(</span><span class="n">r_t</span><span class="p">,</span> <span class="mi">1</span><span class="err">‚àíŒµ</span><span class="p">,</span> <span class="mi">1</span><span class="o">+</span><span class="err">Œµ</span><span class="p">)</span> <span class="o">*</span> <span class="err">ÀÜ</span><span class="n">A_t</span> <span class="p">)</span>

               <span class="o">-</span> <span class="n">Policy</span> <span class="n">loss</span> <span class="p">(</span><span class="n">we</span> <span class="n">minimize</span> <span class="n">negative</span> <span class="n">surrogate</span><span class="p">)</span><span class="o">:</span>
                   <span class="n">L_</span><span class="p">{</span><span class="n">policy</span><span class="p">}</span> <span class="o">=</span> <span class="err">‚àí</span> <span class="n">E_</span><span class="p">{</span><span class="n">t</span><span class="err">‚àà</span><span class="n">B</span><span class="p">}[</span> <span class="n">L</span><span class="o">^</span><span class="p">{</span><span class="n">CLIP</span><span class="p">}</span><span class="n">_t</span><span class="p">(</span><span class="err">Œ∏</span><span class="p">)</span> <span class="p">]</span>

               <span class="o">-</span> <span class="n">Value</span> <span class="n">loss</span> <span class="p">(</span><span class="n">MSE</span><span class="p">),</span> <span class="n">optional</span> <span class="n">value</span> <span class="n">clipping</span><span class="o">:</span>
                   <span class="n">L_</span><span class="p">{</span><span class="n">value</span><span class="p">}</span> <span class="o">=</span> <span class="n">E_</span><span class="p">{</span><span class="n">t</span><span class="err">‚àà</span><span class="n">B</span><span class="p">}[</span> <span class="p">(</span><span class="n">V_new_t</span> <span class="err">‚àí</span> <span class="n">R_t</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="p">]</span>       <span class="err">#</span> <span class="n">optionally</span> <span class="mi">0</span><span class="p">.</span><span class="mi">5</span> <span class="o">*</span> <span class="p">...</span>
                   <span class="p">(</span><span class="n">implementation</span> <span class="n">may</span> <span class="n">use</span> <span class="n">clipped</span> <span class="n">value</span> <span class="n">loss</span> <span class="n">to</span> <span class="n">limit</span> <span class="n">V</span> <span class="n">update</span><span class="p">)</span>

               <span class="o">-</span> <span class="n">Entropy</span> <span class="n">bonus</span><span class="o">:</span>
                   <span class="n">L_</span><span class="p">{</span><span class="n">entropy</span><span class="p">}</span> <span class="o">=</span> <span class="n">E_</span><span class="p">{</span><span class="n">t</span><span class="err">‚àà</span><span class="n">B</span><span class="p">}[</span> <span class="n">S_t</span> <span class="p">]</span>

               <span class="o">-</span> <span class="n">Total</span> <span class="n">loss</span><span class="o">:</span>
                   <span class="n">L</span> <span class="o">=</span> <span class="n">L_</span><span class="p">{</span><span class="n">policy</span><span class="p">}</span> <span class="o">+</span> <span class="n">c1</span> <span class="o">*</span> <span class="n">L_</span><span class="p">{</span><span class="n">value</span><span class="p">}</span> <span class="err">‚àí</span> <span class="n">c2</span> <span class="o">*</span> <span class="n">L_</span><span class="p">{</span><span class="n">entropy</span><span class="p">}</span>

               <span class="o">-</span> <span class="n">Gradient</span> <span class="n">step</span><span class="o">:</span>
                   <span class="err">Œ∏</span> <span class="err">‚Üê</span> <span class="err">Œ∏</span> <span class="err">‚àí</span> <span class="err">Œ±</span> <span class="err">¬∑</span> <span class="n">AdamGrad</span><span class="p">(</span><span class="err">‚àá</span><span class="n">_</span><span class="err">Œ∏</span> <span class="n">L</span><span class="p">)</span>
                   <span class="p">(</span><span class="n">optionally</span> <span class="n">clip</span> <span class="n">gradients</span> <span class="n">by</span> <span class="n">norm</span><span class="p">)</span>

           <span class="n">end</span> <span class="k">for</span> <span class="n">each</span> <span class="n">minibatch</span>
           <span class="p">(</span><span class="n">optional</span><span class="p">)</span> <span class="n">compute</span> <span class="n">approx</span> <span class="n">KL</span> <span class="n">between</span> <span class="err">œÄ</span><span class="n">_</span><span class="p">{</span><span class="err">Œ∏</span><span class="n">_old</span><span class="p">}</span> <span class="n">and</span> <span class="err">œÄ</span><span class="n">_</span><span class="p">{</span><span class="err">Œ∏</span><span class="p">};</span> <span class="k">if</span> <span class="o">&gt;</span> <span class="n">target_kl</span> <span class="n">then</span> <span class="k">break</span> <span class="n">early</span>
    <span class="n">end</span> <span class="k">for</span> <span class="n">epoch</span>

    <span class="mi">5</span><span class="p">.</span> <span class="p">(</span><span class="n">optional</span><span class="p">)</span> <span class="n">logging</span><span class="o">:</span> <span class="n">avg</span> <span class="n">episodic</span> <span class="k">return</span><span class="p">,</span> <span class="n">approx_kl</span><span class="p">,</span> <span class="n">clip</span> <span class="n">fraction</span><span class="p">,</span> <span class="n">explained</span> <span class="n">variance</span><span class="p">,</span> <span class="n">etc</span><span class="p">.</span>

<span class="n">end</span> <span class="k">for</span> <span class="n">iteration</span>

</code></pre></div></div>]]></content><author><name></name></author><category term="sample-posts"/><category term="math"/><category term="code"/><summary type="html"><![CDATA[Policy Gradient Derivation Process]]></summary></entry><entry><title type="html">Supervised Learning</title><link href="https://zane-liao.netlify.app//blog/2025/sul/" rel="alternate" type="text/html" title="Supervised Learning"/><published>2025-09-21T16:52:34+00:00</published><updated>2025-09-21T16:52:34+00:00</updated><id>https://zane-liao.netlify.app//blog/2025/sul</id><content type="html" xml:base="https://zane-liao.netlify.app//blog/2025/sul/"><![CDATA[<h1 id="introduction-to-supervised-learning">Introduction to Supervised Learning</h1> <ul> <li>Supervised learning uses labeled data for training.</li> <li>Therefore, when using supervised learning algorithms, you should pay attention to whether your dataset has been processed (i.e., cleaned, structured, and labeled).</li> <li>Machine learning algorithm tasks are divided into two major categories: regression and classification.</li> <li>Model types are generally categorized as discriminant and generative.</li> <li>Supervised learning algorithms generally include linear regression, as well as its derivatives, logistic regression, Naive Bayes, Support Vector Machine (SVM), Tree Type, and KNN.</li> <li>Linear regression and ridge regression generally have closed-form solutions. In other words, with a closed-form solution, the optimal solution can be obtained using formulas.</li> <li>Logistic regression, SVM, decision trees, and clustering algorithms do not have closed-form solutions; we need to use iteration to find the optimal solution.</li> <li>Models that can directly obtain theoretical optimal solutions using convex optimization tools include linear regression, ridge regression, Lasso, logistic regression, and SVM.</li> <li>Non-convex optimization models: decision trees, K-means, etc.</li> </ul> <h3 id="define">Define</h3> <p>[[1. Supervised Learning Formulations]]</p> <h3 id="regression-and-classification">Regression and Classification</h3> <p>Let‚Äôs first look at the definitions:</p> <ul> <li>Classification is learning a function ùëì: ùëã‚Üíùëå, where the output space ùëå is a discrete finite set</li> <li>Regression is learning a function ùëì: ùëã ‚Üí ùëå, where the output space ùëå ‚äÜ ùëÖ is a subset of the field of continuous real numbers Let:</li> <li>$\mathcal{X}$: input space (features)</li> <li>$\mathcal{Y}$: Output space (labels) <h5 id="classification">Classification</h5> </li> <li><strong>Goal</strong>: Learn $f: \mathcal{X} \to \mathcal{Y}$, where \(\mathcal{Y} = \{1, 2, \dots, K\} \quad (K \ge 2)\) is a finite set (e.g., cat/dog/bird)</li> <li><strong>Loss Function</strong>: The most common is the <strong>0-1 loss</strong>: \(L(f(x), y) = \begin{cases} 0 &amp; \text{if } f(x) = y \\ 1 &amp; \text{otherwise} \end{cases}\) or softmax + cross-entropy loss.</li> <li> <p><strong>Learning Method</strong>: Learn $P(y \mid x)$ or the decision boundary.</p> </li> <li>Classification Goal: $f^*(x) = \arg\max_{y \in \mathcal{Y}} P(y \mid x)$</li> </ul> <h5 id="regression">Regression</h5> <ul> <li><strong>Goal</strong>: Learn $f: \mathcal{X} \to \mathbb{R}$, where</li> </ul> <p>\(\mathcal{Y} \subseteq \mathbb{R}\) is continuous, such as price, temperature, or rent.</p> <ul> <li><strong>Loss Function</strong>: Typically, the mean squared error (MSE) or absolute error (MAE) is used: \(L(f(x), y) = (f(x) - y)^2\)</li> <li> <p><strong>Learning Method</strong>: Learn the expected value $\mathbb{E}[y \mid x]$ or the fitted function value.</p> </li> <li>Regression Goal: $f^*(x) = \mathbb{E}[y \mid x]$</li> </ul> <h5 id="visual-geometry">Visual Geometry</h5> <table> <thead> <tr> <th>Task</th> <th>Data Distribution Image</th> </tr> </thead> <tbody> <tr> <td>Classification</td> <td>Various points are distributed in feature space, and the model needs to draw lines/hyperplanes to separate them.</td> </tr> <tr> <td>Regression</td> <td>Points are distributed along a continuous curve, and the model needs to fit this curve.</td> </tr> </tbody> </table> <p>Source: Bishop, ‚ÄúPattern Recognition and Machine Learning‚Äù: Classification involves discrete labels, regression involves real-valued targets.</p> <p>Hastie, Tibshirani, Friedman, ‚ÄúThe Elements of Statistical Learning‚Äù: The goal of regression is to predict a continuous response; for classification, the response is categorical.</p> <hr/> <h3 id="discriminant-and-generative-models">Discriminant and Generative Models</h3> <ul> <li> <p>Discriminant models are concerned with ‚Äúgiven an email, is it spam?‚Äù We learn the joint probability $P(y \mid x)$, which simultaneously models the input x and the output. The generation mechanism of y</p> </li> <li> <table> <tbody> <tr> <td>Generative models focus on the question ‚ÄúHow is a spam email written?‚Äù and directly learn the conditional probability distribution ùëÉ(ùë¶</td> <td>ùë•) or the decision boundary ùëì(ùë•)‚Üíùë¶ to achieve classification or regression tasks.</td> </tr> </tbody> </table> </li> </ul> <h5 id="linear-regression">Linear Regression</h5> <ul> <li> <p>Linear regression is the first supervised learning algorithm we learn. While it may seem simple at first glance, it‚Äôs actually quite complex due to its long history and widespread use in various fields. Many variants have emerged, making it quite complex. If you‚Äôre new to machine learning, it‚Äôs easy to be overwhelmed by this model. So, if you‚Äôre asked in an interview which model you‚Äôre most familiar with, avoid saying you‚Äôre familiar with linear regression. (ü§ì)</p> </li> <li> <p>Linear regression mainly includes the simplest Simple Linear Regression, the derived Multiple Linear Regression, Least Squares Linear Regression (OLS), and Generalized Linear Models (GLMs, for example, a variant is the Poisson Regression). Regression), linear regression with added L1 (Ridge) and L2 (Lasso) regularization terms (OLS + L2 = Ridge Regression), recursive least squares linear regression (RLS) for time series, nonlinear kernel regression (Kernel method), principal component regression (PCR), linear regression from a Bayesian perspective, etc.</p> </li> <li> <p>Linear regression has both discriminant and generative versions (Gaussian Joint Modeling)</p> </li> </ul> <h5 id="logistic-regression">Logistic Regression</h5> <ul> <li> <p>Logistic regression can be loosely considered linear regression for classification tasks.</p> </li> <li> <p>Logistic regression includes the simplest standardized logistic regression, logistic regression with added L1 and L2 terms (same as above), elastic net logistic regression (L1 + L2 = Elastic Net), softmax regression, kernel logistic regression (same as above), logistic regression from a Bayesian perspective (same as above), online logistic regression (SGD, AdaGrad), etc.</p> </li> <li> <p>Logistic regression also has both discriminant and generative versions (Gaussian Discriminant Analysis (GDA is a generative variant of logistic regression)</p> </li> </ul> <p><strong>Linear Regression and Logistic Regression are characterized by strong interpretability, fast training, and good accuracy, but are also susceptible to outliers</strong></p> <h5 id="naive-bayes">Naive Bayes</h5> <ul> <li> <p>Naive Bayes is a generative classification model. Why is it called ‚ÄúNaive‚Äù? Because all features are independent given the known class y.</p> </li> <li> <p>Naive Bayes is not a single model. It models $P(xi \mid y)$ based on a conditional distribution assumption. The conditional distribution is a large family of exponential distributions. The distribution to use depends on the input data type. For example, for text word frequency classification, we use a multinomial distribution (NB), while for image feature classification, we use a Gaussian NB or Bernoulli NB.</p> </li> <li> <p>Naive Bayes is characterized by fast training, the assumption of conditional independence, and robustness when data is scarce.</p> </li> </ul> <h5 id="support-vector-machine-svm">Support Vector Machine (SVM)</h5> <ul> <li> <p>Support Vector Machine (SVM) is a linear discriminant model that finds a hyperplane in feature space that maximizes the geometric margin to optimally distinguish between two classes of data.</p> </li> <li>Margin: The distance from the hyperplane to the closest sample point.</li> <li>Support Vectors: The sample points $y^{(i)} (w^T x^{(i)} + b) = 1$ that are closest to the hyperplane and satisfy the bounds.</li> <li> <p>SVM is essentially a maximum margin classifier.</p> </li> <li> <p>Real data is often linearly inseparable or noisy, so we cannot use a hard margin. Instead, we introduce a slack variable $\xi_i \ge 0$ (allowing some samples to cross the bounds).</p> </li> <li>The original SVM problem is a convex optimization problem, which is more suitable for computing kernel functions.</li> <li>SVM can be extended to nonlinear spaces by using the kernel function K(x,x‚Ä≤) instead of the inner product.</li> </ul> <h6 id="kernel-method">Kernel Method</h6> <ul> <li> <p>Kernel methods implicitly map the original input to a high-dimensional feature space by defining a kernel function K(x,x‚Ä≤), thereby constructing a linear model in this space without explicitly performing feature mapping.</p> </li> <li> <p>Why do we need kernel methods? Many data are linearly inseparable in the original space, but may be linearly separable in a higher-dimensional space. However, in a higher-dimensional space, the computational cost is very high or even impossible. We introduce an implicit mapping (calculating the inner product $\phi(x)^T \phi(x‚Äô)$) and avoid directly constructing a high-dimensional space (directly using $K(x, x‚Äô)$).</p> </li> <li> <p>Mathematical definition: A function $K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is a kernel function if and only if there exists a <strong>Hilbert space</strong> $\mathcal{H}$ and a mapping $\phi: \mathcal{X} \rightarrow \mathcal{H}$ such that: \(K(x, x') = \langle \phi(x), \phi(x') \rangle_{\mathcal{H}}\)</p> </li> <li> <p>Kernel functions typically come in many forms, including linear kernels, polynomial kernels, Gaussian kernels (RBF kernels), and sigmoid kernels.</p> </li> <li>An example is the original dual form of SVM: $f(x) = \text{sign}\left( \sum_{i=1}^m \alpha_i y^{(i)} \langle x^{(i)}, x \rangle + b \right)$</li> <li> <p>After replacing the inner product with a kernel function: $f(x) = \text{sign}\left( \sum_{i=1}^m \alpha_i y^{(i)} K(x^{(i)}, x) + b \right)$</p> </li> <li> <p>Common types of kernel methods include SVM with kernel, Kernel PCA, Kernel Ridge Regression, and Kernel K-means.</p> </li> <li>Given a set of samples, the kernel matrix (Gram Matrix) $x^{(1)}, \dots, x^{(n)}$, definition: $K_{ij} = K(x^{(i)}, x^{(j)})$ then $K \in \mathbb{R}^{n \times n}$ is a positive semidefinite matrix and must satisfy the following: <ol> <li>For any $\alpha \in \mathbb{R}^n$, $\alpha^T K \alpha \ge 0$</li> <li>This is the core of Mercer‚Äôs theorem: the kernel function must be positive definite.</li> </ol> </li> </ul> <h5 id="covariance-matrix">Covariance Matrix</h5> <ul> <li> <p><strong>Covariance Matrix</strong> describes how multiple variables change together: the degree of correlation between each pair of variables is described by covariance, and the ‚Äújoint correlation‚Äù of the entire set of variables constitutes the covariance matrix</p> </li> <li> <p><strong>Math Define</strong>: Suppose you have $m$ samples, each sample is an $n$-dimensional vector (that is, you have $n$ variables): \(X = \begin{bmatrix} \text{---} (x^{(1)})^T \text{---} \\ \text{---} (x^{(2)})^T \text{---} \\ \vdots \\ \text{---} (x^{(m)})^T \text{---}\end{bmatrix} \in \mathbb{R}^{m \times n}\) Sample mean vector: \(\mu = \frac{1}{m} \sum_{i=1}^m x^{(i)} \in \mathbb{R}^n\) <strong>Covariance matrix</strong> $\Sigma \in \mathbb{R}^{n \times n}$ is defined as: \(\Sigma = \frac{1}{m} \sum_{i=1}^m (x^{(i)} - \mu)(x^{(i)} - \mu)^T = \mathbb{E}\left[(x - \mu)(x - \mu)^T\right]\) Can also be written in matrix form (sample matrix $X$, one sample per row): \(\Sigma = \frac{1}{m} (X - \mu)^T (X - \mu)\)</p> </li> <li>We know that $\Sigma_{ij}$ represents the covariance between the $i$th variable and the $j$th variable: \(\Sigma_{ij} = \text{Cov}(x_i, x_j) = \mathbb{E}[(x_i - \mu_i)(x_j - \mu_j)]\) So:</li> <li>$\Sigma_{ii} = \text{Var}(x_i)$: variance of variable $x_i$</li> <li>$\Sigma_{ij} &gt; 0$: positive correlation</li> <li>$\Sigma_{ij} &lt; 0$: negative correlation</li> <li>$\Sigma_{ij} = 0$: uncorrelated (but not necessarily independent)</li> </ul> <h6 id="visual-geometry-1">Visual Geometry</h6> <ul> <li>The covariance matrix describes the ‚Äúscatter‚Äù of data points in all directions.</li> <li> <p>It can be thought of as the <strong>shape outline</strong> of the data. * If you plot a 2D data point cloud, the <strong>eigenvector</strong> of the covariance matrix gives the principal direction, and the <strong>eigenvalue</strong> gives the variance (length) in that direction ‚Üí Principal Component Analysis (PCA) selects the principal eigenvector of the covariance matrix as the projection direction.</p> </li> <li>The Covariance Matrix generally satisfies <ol> <li>$\Sigma = \Sigma^{T}$ (symmetry)</li> <li>$v^T \Sigma v \ge 0 \, \forall v$ (positive semidefinite)</li> <li>$\lambda_i \ge 0$ (all eigenvalues ‚Äã‚Äãare non-negative)</li> <li>Positive definite (if all samples are independent)</li> <li>Diagonalizable (orthogonal eigenvectors are diagonalized), etc.</li> </ol> </li> <li>Application Scenarios: PCA GDA Gaussian Process Multiple Variable Normal Distribution Model Whitening et al.</li> </ul> <p>For Example:</p> <ul> <li>Suppose we have 2 variables:</li> </ul> <p>| x‚ÇÅ | x‚ÇÇ | | ‚Äî | ‚Äî | | 1 | 2 | | 2 | 4 | | 3 | 6 | | 4 | 8 | x‚ÇÇ = 2 * x‚ÇÅ =&gt; linear correlation</p> <p>Compute the Covariance Matrix:</p> <p>\(\Sigma = \begin{bmatrix} \text{Var}(x_1) &amp; \text{Cov}(x_1, x_2) \\ \text{Cov}(x_2, x_1) &amp; \text{Var}(x_2) \end{bmatrix} = \begin{bmatrix} 1.67 &amp; 3.33 \\ 3.33 &amp; 6.67 \end{bmatrix}\)</p> <h6 id="why-can-the-covariance-matrix-be-used-directly-in-a-movie-recommendation-system-based-on-collaborative-filtering">Why can the Covariance Matrix be used directly in a movie recommendation system based on collaborative filtering?</h6> <p>Basic Idea: In a recommendation system, if the covariance between the ratings of two movies is high, it means that if a user likes movie A, they are also likely to like movie B (or that their ratings have the same trend). These movies are moving in the same direction in the ‚Äúuser rating space,‚Äù so we can use the covariance to determine the similarity between the movies (covariance = similarity).</p> <h5 id="tree-type">Tree Type</h5> <ul> <li>Tree algorithms are widely used in various fields, from regression and classification to ranking and anomaly detection.</li> <li>There are many tree-type algorithms, which can be roughly divided into two categories: simple trees and ensemble trees.</li> <li>The simplest tree is the decision tree, with variants like CART (Classification and Regression Tree).</li> <li>Decision trees can be used for regression tasks.</li> <li>Ensemble trees include Random Forest (an ensemble of multiple CART trees) and Boosted Trees. Boosted Trees include Gradient Boosting Decision Trees (GBDT), XGBoost, LightGBM, CatBoost, AdaBoost, etc.</li> </ul> <h5 id="knn">KNN</h5> <ul> <li>Simply put, the KNN (nearest neighbor) algorithm is a parameter-free, distance-based classification or regression method. It‚Äôs a type of lazy learning algorithm and is commonly used in classification and regression tasks. It‚Äôs suitable for small sample sizes and nonlinear problems, but it‚Äôs inefficient in large-scale data or high-dimensional spaces and requires optimization.</li> </ul> <h5 id="kd-tree">KD-Tree</h5> <ul> <li>Again, briefly, the KD-Tree is not an algorithm, but rather a special binary tree data structure.</li> <li>A strict definition: <strong>KD-Tree is a data structure that supports fast multidimensional space retrieval</strong>. It‚Äôs used for nearest neighbor searches and range searches in $\mathbb{R}^k$.</li> <li>The KD-Tree is primarily used in the KNN algorithm to quickly find the nearest neighbor.</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="math"/><category term="code"/><summary type="html"><![CDATA[Introduction to Machine Learning]]></summary></entry><entry><title type="html">Deep Learning</title><link href="https://zane-liao.netlify.app//blog/2025/deep/" rel="alternate" type="text/html" title="Deep Learning"/><published>2025-09-16T18:34:00+00:00</published><updated>2025-09-16T18:34:00+00:00</updated><id>https://zane-liao.netlify.app//blog/2025/deep</id><content type="html" xml:base="https://zane-liao.netlify.app//blog/2025/deep/"><![CDATA[<h1 id="introduction-to-deep-learning">Introduction to Deep Learning</h1> <ul> <li>Deep learning, a training method that combines computing power and data, has been widely used since the use of GPUs to train AlexNet on ImageNet in 2012.</li> <li>We need to understand what deep learning is and how it differs from traditional machine learning.</li> <li>Deep learning is a combination of deep neural networks and architecture = deep learning models.</li> <li>Although deep learning is highly effective, its interpretability is poor and it is currently almost a black box.</li> <li>Deep learning relies on the amount of data available. For real-world tasks, if data is insufficient, it is best not to use deep learning to train from scratch, as this will result in performance inferior to traditional machine learning algorithms. Choose the most suitable model based on the size and distribution of the dataset.</li> <li>Deep learning is non-convex, meaning that we may not find the theoretical ‚Äúglobal optimal solution.‚Äù In practice, trained networks often reach local optimal solutions, but local optimal solutions in deep neural networks often perform well.</li> <li>The primary optimization algorithm in deep learning is backpropagation.</li> <li>In terms of scale and number of layers, neural networks with more than one layer are theoretically called deep neural networks (DNNs). In practice, there is no clear distinction.</li> </ul> <h5 id="neural-network">Neural Network</h5> <ul> <li>A neural network is a combination of linear transformation and activation function.</li> <li>Formula: $\mathbf{h} = \sigma(W \mathbf{x} + b)$</li> <li>Without an activation function, a multi-layer model degenerates into a single-layer linear model.</li> <li>The key to success in deep learning: nonlinear activation functions + stacking.</li> <li>Common activation functions include: Tanh, ReLU, Sigmoid, Softmax, etc.</li> <li>Currently, the Pytorch framework is generally used as a launchpad for training models in the deep learning field, while the TensorFlow framework is gradually fading from mainstream use.</li> </ul> <h5 id="forward-propagation">Forward Propagation</h5> <ul> <li>Forward propagation simply involves input (usually converted to a matrix) -&gt; linear transformation + activation function -&gt; output.</li> <li>Forward propagation does not involve gradient updates or parameter updates. <bt></bt></li> </ul> \[\begin{align*} z1 = W1x + b1 \\ a1 = œÉ(z1) \\ z2 = W2a1 + b2 \\ ≈∑ = œÉ_{out}(z2) \end{align*}\] <h5 id="back-propagation">Back Propagation</h5> <ul> <li>Backpropagation is the most important optimization algorithm in deep learning.</li> <li>When training neural networks, we generally use the chain rule to calculate the gradient of the loss function with respect to the parameters to update the weights.</li> <li>The calculation process is generally forward to obtain the output $≈∑$ -&gt; calculate the loss -&gt; calculate layer by layer -&gt; parameter update</li> <li>Essence: propagate the error layer by layer and use gradient descent to optimize the parameters</li> <li>Let‚Äôs take an example <ol> <li>Input Layer: $x$</li> <li>Hidden Layer: $z^{[1]} = W^{[1]}x + b^{[1]}, \quad a^{[1]} = \sigma(z^{[1]})$</li> <li>Output Layer: $z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}, \quad \hat{y} = \sigma(z^{[2]})$</li> <li>Loss Function: $L = \frac{1}{2}(y - \hat{y})^2$</li> </ol> </li> <li>Back Propagation step: <ol> <li>Output Error: $\delta^{[2]} = \frac{\partial L}{\partial z^{[2]}} = (\hat{y} - y) \cdot \sigma‚Äô(z^{[2]})$</li> <li>Hidden Error: $\delta^{[1]} = \frac{\partial L}{\partial z^{[1]}} = (W^{[2]})^T \delta^{[2]} \cdot \sigma‚Äô(z^{[1]})$</li> </ol> </li> <li>Gradient Compute: <ol> <li>$\frac{\partial L}{\partial W^{[2]}} = \delta^{[2]} (a^{[1]})^T$</li> <li>$\frac{\partial L}{\partial b^{[2]}} = \delta^{[2]}$</li> <li>$\frac{\partial L}{\partial W^{[1]}} = \delta^{[1]} x^T$ $\frac{\partial L}{\partial b^{[1]}} = \delta^{[1]}$</li> </ol> </li> <li>The specific details of backpropagation are not explained. We only need to know that the goal of the neural network is to minimize the loss function. Without the backpropagation algorithm, using other algorithms (such as numerical algorithms) requires a complexity of $O(n^2)$. With backpropagation, we can achieve the same complexity as forward propagation, $O(n)$.</li> <li>Without backpropagation, it is difficult to train large-scale neural networks.</li> </ul> <h5 id="optimization-algorithms">Optimization Algorithms</h5> <ul> <li>Gradient Descent (GD) uses all samples to update parameters each time. It is very inefficient when the amount of data is large.</li> <li>Stochastic Gradient Descent (SGD) uses one sample to update parameters each time. It is fast to calculate and can jump out of the local optimum, but it has large fluctuations and is not easy to converge.</li> <li>Mini-batch Gradient Descent uses a small batch of samples (batch) to update parameters each time. It is a compromise between efficiency and stability.</li> <li>Momentum $v = Œ≤v + (1-Œ≤)‚àáL Œ∏ = Œ∏ - Œ∑v$ simulates inertia, reduces oscillations, and accelerates convergence.</li> <li>Adagrad uses adaptive learning rates to reduce frequent learning rate updates, but its drawback is that small learning rates can lead to premature stagnation.</li> <li>RMSProp uses weighted averaging to improve the rapid decay of Adagrad‚Äôs learning rate.</li> <li>Adam (Adaptive Moment Estimation) combines momentum with RMSProp and is the most commonly used optimizer.</li> <li>AdamW introduces weight decay to Adam, resulting in more stable results and is a commonly used optimizer for LLM.</li> <li>Others: AdaMax, Nadam, LAMB, MuonClip, etc.</li> </ul> <h5 id="regularization-methods">Regularization Methods</h5> <ul> <li>Regularization methods are generally used to prevent model overfitting and improve robustness and generalization.</li> <li>Regularization methods are generally categorized into L1 and L2, dropout, data augmentation, early stopping, batch normalization, and layer normalization.</li> <li> <table> <tbody> <tr> <td>L1 is Lasso, which makes the parameters sparse: $\lambda\sum_i</td> <td>wi</td> <td>$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>L2 is Ridge, which makes weights smaller but not sparse: $\lambda\sum_i</td> <td>wi</td> <td>$</td> </tr> </tbody> </table> </li> <li>Dropout randomly drops neurons during training.</li> <li>Data Augmentation modifies the model to make it more robust. Methods include rotation, translation, flipping, and adding noise.</li> <li>Early Stopping monitors the loss and stops training if performance starts to decline.</li> <li>Batch Normalization/Layer Normalization normalizes the outputs of intermediate layers to reduce exploding and vanishing gradients. A small amount of regularization is more like an optimization technique.</li> <li>Why are Regularization Methods rarely used for training LLMs? LLMs require a very large amount of data and often suffer from underfitting. Furthermore, optimizers like the AdamW optimizer use a small amount of regularization and weight decay.</li> </ul> <h5 id="initialization-methods">Initialization Methods</h5> <ul> <li>Keep the activations and gradient variances of each layer as stable as possible during forward and backward propagation to prevent vanishing and exploding gradients, making it easier to train deep networks.</li> <li>Random Initialization: Both Gaussian and uniform distributions are acceptable.</li> <li>Uniform: $W_{ij}\sim \mathcal{N}(0,\sigma^2)$</li> <li>Gaussian: $W_{ij}\sim \mathcal{U}[-a,a] \mathrm{Var}=a^2/3$</li> <li>He Initialization (Kaiming Initialization)</li> <li>LeCun Initialization</li> <li>Xavier Initialization (Glorot Initialization), etc.</li> </ul> <h5 id="convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)</h5> <ul> <li>CNN is the core model for processing grid data such as <strong>images, videos, and speech</strong> in deep learning.</li> <li>Compared to fully connected networks, it has fewer parameters and better preserves spatial structure.</li> <li>Convolutional neural networks were proposed by Le. Their general layer architecture is: convolution layer + activation function + pooling layer + fully connected layer + normalization layer.</li> <li>The convolution layer performs a local weighted sum on the input feature map using a convolution kernel (filter/kernel). <br/> \(\begin{align*}y_{i,j,k} = \sum_{c=1}^{C} \sum_{m=1}^{M} \sum_{n=1}^{N} x_{i+m, j+n, c} \cdot w_{m,n,c,k}\end{align*}\)</li> <li>Parameters: kernel size (M√óN), stride, padding</li> <li>Activation functions: ReLU, Leaky ReLU, GELU, etc., which increase nonlinearity and can learn complex parameters from the data</li> <li>Pooling layers: MaxPooling / AveragePooling, which reduce image size and computational complexity</li> <li>Fully connected layers: After convolutional feature extraction, fully connected layers perform regression or classification</li> <li>Norm layers: Perform normalization, add block convergence, and alleviate vanishing gradients</li> <li>CNN Architectures: LeNet, AlexNet, VGGNet, ResNet, DenseNet, GoogleNet, etc.</li> <li>The most important CNN architecture is ResNet. By introducing residual connections, it solves the vanishing gradient problem in deep neural networks. Formula: $y = F(x, {W_i}) + x$</li> <li>CNNs are generally used for image classification, object detection, semantic segmentation, medical image analysis, etc.</li> <li>CNN-derived architectures and algorithms include U-Net and YOLO.</li> </ul> <h5 id="recurrent-neural-networks-rnn-gru-lstm">Recurrent Neural Networks (RNN, GRU, LSTM)</h5> <ul> <li>These models are generally used for NLP, speech recognition, time series, video understanding, etc. <h6 id="rnn">RNN</h6> </li> <li>RNN, also known as recurrent neural network, is a neural network used to process sequential data (text, time series, speech, etc.).</li> <li>Main Idea: <strong>Hidden State</strong> acts as memory, transferring historical information to the next moment.</li> <li>RNNs are prone to problems such as vanishing/exploding gradients.</li> <li>Improvements: GRU (Gated Recurrent Unit) LSTM (Long Short-Term Memory Network) \(\text{Given $x_t$, Hidden state update: }\ h_t = \tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\) \(\text{Output: }\ y_t = W_{hy} h_t + b_y\) <h6 id="gru">GRU</h6> </li> <li>Compared to RNNs, GRUs add <strong>memory cells (cell states)</strong> and <strong>gates</strong>, which effectively mitigate the vanishing/exploding gradient problem.</li> <li>Can model long-term dependencies, but training is slow and requires a large number of parameters.</li> <li>Input: $x_t$, Previous state: $h_{t-1}, c_{t-1}$</li> <li>Input gate: \(i_t = \sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i)\)</li> <li>Forget gate: \(f_t =\sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f)\)</li> <li>Candidate Memory: \(\tilde{c}_t = \tanh(W_{xc} x_t + W_{hc} h_{t-1} + b_c)\)</li> <li>State: \(c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t\)</li> <li>Output Gate: \(o_t = \sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o)\)</li> <li>Hidden State: \(h_t = o_t \odot \tanh(c_t)\)</li> </ul> <h6 id="lstm">LSTM</h6> <ul> <li>Alleviating vanishing/exploding gradient issues in RNNs during training and capturing long-term dependencies</li> <li>Main Idea: Introducing Additive Cells The state $c_t$ and the gate $(i,f,o)$ allow the gradient to flow along $c_t$ almost ‚Äúunimpeded.‚Äù</li> <li>A simple reference implementation \(\begin{bmatrix} i_t \\ f_t \\ g_t \\ o_t \end{bmatrix} = \begin{bmatrix} \sigma \\ \sigma \\ \tanh \\ \sigma \end{bmatrix} \left( W_x x_t + W_h h_{t-1} + b \right)\)</li> <li>Update the state \(c_t = f_t \odot c_{t-1} + i_t \odot g_t\) \(h_t = o_t \odot \tanh(c_t)\)</li> <li>$W_x \in \mathbb{R}^{4h \times d}$ , $W_h \in \mathbb{R}^{4h \times h}$, $b \in \mathbb{R}^{4h}$</li> </ul> <h5 id="self-supervised-learning">Self-Supervised Learning</h5> <ul> <li>Before moving on to Transformers, we must first understand what self-supervised learning is.</li> <li>Self-supervised learning (SSL) is a learning method that does not rely on human annotations.</li> <li>Learning is achieved by constructing supervisory signals from the data itself.</li> <li>SSL does not require human labels, but ‚Äúpseudo-labels‚Äù are constructed during training.</li> <li>SSL can be considered a combination of unsupervised and supervised learning.</li> <li>Main idea: Design a pre-training task to construct ‚Äúpseudo-labels,‚Äù for example, predicting masked words given a text sequence (BERT).</li> <li>Representation Learning: Learn the feature space through these tasks.</li> <li>Downstream tasks: Transfer the learned representations to tasks such as classification, regression, and generation.</li> <li>Common methods:</li> <li>Predictive SLL:</li> </ul> <ol> <li>Masked Language Modeling (MLM, e.g., BERT)</li> <li>Masked Autoencoder (MAE for vision)</li> <li>Sequence Auto-regression (GPT) <ul> <li>Contrastive SSL:</li> </ul> </li> <li>SimCLR, MoCo, BYOL, SimSiam <ul> <li>Generative SSL</li> </ul> </li> <li>Autoencoder</li> <li>Variational Autoencoder (VAE)</li> <li>Diffusion-based SSL (e.g., Masked Diffusion Pretraining)</li> </ol> <ul> <li>SimCLR (Chen et al., Google Research, 2020) is a self-supervised contrastive learning method that does not rely on labels and learns representations by maximizing the consistency of different augmented views.</li> <li>InfoNCE Loss is: <br/></li> </ul> \[\begin{align*} L_{\text{pre}}(\theta) = - \sum_{i=1}^{B} \log \frac{\exp \left( \phi_{\theta}(\hat{x}^{(i)})^{\top} \phi_{\theta}(\tilde{x}^{(i)}) \right)} {\exp \left( \phi_{\theta}(\hat{x}^{(i)})^{\top} \phi_{\theta}(\tilde{x}^{(i)}) \right) + \sum_{j \neq i} \exp \left( \phi_{\theta}(\hat{x}^{(i)})^{\top} \phi_{\theta}(\tilde{x}^{(j)}) \right)} \end{align*}\] <h5 id="transformer">Transformer</h5> <ul> <li>As one of the most important model architectures in recent years, the Transformer has demonstrated excellent performance in tasks such as NLP, Computer Vision, and speech. This is especially true for the Generative Prediction (GPT) architecture derived from the Transformer.</li> <li>I won‚Äôt go into detail about the Transformer architecture; there are excellent resources online <a href="https://stanford-cs336.github.io/spring2025/">cs336</a></li> <li>In the 2017 Transformer paper, the core architecture is (Encoder+Decoder)</li> <li>The core is Self-Attention+FFN+Residual+Norm</li> <li>The Encoder extracts contextual information, and the Decoder autoregressively predicts the sequence.</li> <li>The core of Attention: \(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</li> <li>Modern LLMs are all decoder-only architectures, eliminating the original encoder and simplifying them to autoregressive language models.</li> <li>As of August 2025, the typical combination of modern LLM architectures is RMSNorm+RoPE+FFN(SwiGLU)</li> <li>LayerNorm+Learned has been replaced. Embedding + FFN (ReLU/GeLU)</li> <li>Attention variants include new architectures like FlashAttention that optimize I/O and accelerate computation.</li> <li>For a detailed introduction, see the CS336 lecture notes.</li> <li>MoE (Mixture of Experts) architecture: Multiple FFN blocks are run in parallel, but only k experts are activated at a time. \(y = \sum_{i \in \text{TopK}(G(x))} g_i(x) \cdot E_i(x)\)</li> <li>$E_i$ Experts(FFN)</li> <li>$G(x)$ Gating network, determines which expert to route to</li> <li> <p>$g_i(x)$ Experts‚Äô weights</p> </li> <li>Multimodal Transformer: Language + Vision + Audio + Code ‚Üí Multimodal</li> <li>Common architectures include single-tower, dual-tower, LLM + Adapter, and foundation multimodal LLM.</li> </ul> <h5 id="mamba">Mamba</h5> <ul> <li>A major success of the Transformer is its ability to parallelize and distribute computations, significantly improving training and inference efficiency. However, a drawback is that computational efficiency degrades when dealing with extremely long sequences, with a time complexity of $O(L^2 \cdot d)$. It also consumes a lot of memory, which is why LLMs require large amounts of video memory and RAM, and are very I/O-intensive.</li> <li>The Mamba architecture is based on the Structured State Space Model (SSM).</li> <li>Its core lies in the introduction of the Selective State Space (SSS), which enables modeling of long sequences.</li> <li>Its time complexity is $O(L \cdot d)$.</li> <li>For details on the Mamba model, please refer to <a href="https://arxiv.org/abs/2312.00752">Mamba</a>.</li> </ul> <h5 id="deep-generative-models">Deep Generative Models</h5> <ul> <li>What is a Deep Generative Model?</li> <li>It learns the probability distribution $P(x)$ or conditional distribution $P(x \mid y)$ of the data and generates new samples similar to the training data.</li> <li>Autoregressive Models, Energy-Based Models (EBM)</li> <li>Image Generation: GAN, VAE, Diffusion Model</li> <li>Text Generation (GPT Series)</li> <li>Data Augmentation and Simulation</li> <li>I know very little about deep generative models, but if you want to delve deeper, the best resources and lecture notes are <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8">cs236</a> and <a href="https://cs231n.stanford.edu/">cs231n</a> ![[Screenshot 2025-09-20 at 11.54.48.png]]</li> </ul> <h5 id="gan">GAN</h5> <ul> <li>Training Two Networks‚ÄîGenerator $G$ and Discriminator $D$</li> <li>Goal: \(\min_G \max_D \mathbb{E}_{x \sim p_\text{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log (1 - D(G(z)))]\)</li> <li>GAN variants include CD-GAN and Style-GAN. The difficulty in training GANs lies in the balance between the generator and the discriminator. ![[Screenshot 2025-09-20 at 14.33.21.png]] ![[Screenshot 2025-09-20 at 14.59.47.png]] ![[Screenshot 2025-09-20 at 14.59.58.png]]</li> <li>Training Step ![[Screenshot 2025-09-20 at 15.04.58.png]]</li> <li>Some Questions with the Training Model ![[Screenshot 2025-09-20 at 15.49.13.png]] ![[Screenshot 2025-09-20 at 15.49.00.png]]</li> </ul> <h5 id="vae">VAE</h5> <ul> <li>Maps data to a latent space $z$ and then reconstructs data from the latent space</li> <li>Encoder: $q_\phi(z \mid x)$</li> <li>Decoder: $p_\theta(x \mid z)$</li> <li>Goal (ELBO): \(\mathcal{L}(\theta, \phi; x) = \mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x \mid z)] - \text{KL}(q_\phi(z \mid x) \| p(z))\)</li> <li>One of the best lectures on this topic is <a href="https://www.youtube.com/watch?v=zbHXQRUNlH0&amp;list=PLoROMvodv4rOmsNzYBMe0gJY2XS8AQg16&amp;index=13">cs231n</a> ![[Screenshot 2025-09-19 at 22.38.29.png]] ![[Screenshot 2025-09-19 at 22.38.40 1.png]] ![[Screenshot 2025-09-19 at 22.38.05.png]] ![[Screenshot 2025-09-19 at 22.37.49 1.png]]</li> </ul> <h5 id="diffusion-model">Diffusion Model</h5> <ul> <li>Generates samples by gradually adding noise to the data and then learning the inverse process of removing the noise.</li> <li>Diffusion This model is also useful for generating text code for NLP. Some progress has been made. For details, please refer to the Google paper.</li> <li>Forward (+noise): $q(x_t \mid x_{t-1})$</li> <li>Backward (-noise): Learning $p_\theta(x_{t-1} \mid x_t)$</li> <li>Goal (ELBO) \(\mathcal{L}_{\text{ELBO}} = \mathbb{E} \Big[ \sum_{t=1}^T D_{KL}(q(x_t \mid x_{t-1}) \| p_\theta(x_{t-1} \mid x_t)) \Big]\)</li> <li>Diffusion models are a complex class of models. According to the instructor of cs231n, there are three different mathematical formulations for derivation.derivation and notation, and a 5-page derivation</li> <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lilian weng‚Äôs blog</a></li> <li><a href="https://sander.ai/2023/07/20/perspectives.html">Sander Dielman‚Äôs blog</a> -Some Paper: <ol> <li><a href="https://arxiv.org/pdf/1503.03585">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a></li> <li><a href="https://arxiv.org/pdf/2006.11239">Denoising Diffusion Probabilistic Models</a></li> <li><a href="https://arxiv.org/pdf/2011.13456">SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS</a></li> </ol> </li> <li>High-level Overview ![[Screenshot 2025-09-20 at 16.42.47.png]]</li> <li>Intuitively, rectified flow models are a type of diffusion model. ![[Pasted image 20250920183611.png]] ![[Screenshot 2025-09-20 at 18.01.57.png]] ![[Screenshot 2025-09-20 at 18.02.04.png]] ![[Screenshot 2025-09-20 at 18.54.21.png]] ![[Screenshot 2025-09-20 at 18.54.50.png]] ![[Screenshot 2025-09-20 at 18.55.06.png]] ![[Screenshot 2025-09-20 at 19.33.50.png]] ![[Screenshot 2025-09-20 at 19.32.49.png]] ![[Screenshot 2025-09-20 at 19.37.12.png]] ![[Screenshot 2025-09-20 at 19.37.26.png]]</li> <li>Modern Diffusion Model The most common one is Latent Diffusion Model (LDM) is essentially a combination of VAE, GAN, and Diffusion. Why? Because using modern diffusion models for tasks requires high-quality, well-generated data, which is the advantage of GANs. They can quickly generate latent spaces, which is the advantage of VAEs. ![[Screenshot 2025-09-20 at 20.21.40.png]] ![[Screenshot 2025-09-20 at 20.21.53.png]] <h5 id="diffusion-transformer">Diffusion transformer</h5> <p>![[Screenshot 2025-09-20 at 20.27.25.png]]</p> </li> <li>Some examples ![[Screenshot 2025-09-20 at 20.32.08.png]] ![[Screenshot 2025-09-20 at 20.32.19.png]] ![[Screenshot September 20, 2025, at 20.40.29.png</li> </ul> <p>‚Ä¶For details, please see cs231n Lecture 14.</p> <h5 id="llm-inference">LLM Inference</h5> <ul> <li>We typically use Softmax to determine the probability of each token in the vocabulary, select the one with the highest probability (greedy search) and output it (using Tokenizer decode() to decode the token ID into a token). This token ID is then fed back into the model to predict the next token, looping until $<EOS>$ is generated or the maximum length is reached (the maximum number of tokens output is typically set in config.yaml, for example, 2048 tokens). This step is typically called autoregression.</EOS></li> <li>If the maximum number of tokens output is not set, issues such as increased latency, increased GPU memory usage, and duplicate model output can occur.</li> <li>For multimodal LMs, such as images, the output tokens are typically latent codes, which are converted back to pixel images by the Decoder.</li> <li>The vector dimensions of visual and textual tokens must be consistent to be able to be placed in the same Transformer.</li> <li>To speed up inference, we typically use a KV Cache.</li> <li>The core of the KV Cache technique is to store the key-value pairs (KVs) calculated after each attention layer is computed. This saved KV is then used each time the model is inferred, rather than recalculating the previous KVs. Cache: ```python</li> </ul> <p>cache[layer].k = [k1, k_k_new] cache[layer].v = [v1, v_new] ```</p> <ul> <li>Without using a KV cache, the time complexity is $O(N^{2} \cdot d)$. With a KV cache, the time complexity is reduced to $O(N \cdot d)$.</li> <li>We typically use open-source libraries for LM output (HuggingFace Transformer, Ollama, etc.). This allows for generating a token and immediately returning it to the front-end interface or CLI. We call this step token-by-token streaming.</li> <li>vLLM=&gt;very fast LLM. This is an open-source inference engine specifically designed for LLM inference acceleration, supporting multi-user and high-concurrency scenarios. Its core technology is PageAttention, which addresses the problems of traditional KV caches: video memory fragmentation and memory waste. vLLM stores the KV cache as pages, rather than arrays. We can think of it as KV The cache implements ‚ÄúVirtual Memory‚Äù to support high concurrency. Compared to traditional one-shot batching, it can be interleaved, resulting in significantly higher concurrent throughput.</li> <li>From a time complexity perspective, for an autoregressive model, the total complexity is $O(L \cdot N^{3} \cdot d)$, where L is the number of layers, N is the input sequence length, and d is the hidden dimension. Using only the KV Cache reduces the complexity to $O(L \cdot N^{2} \cdot d)$. Adding vLLM significantly reduces memory usage, while maintaining the overall complexity. Because P ¬´¬†N, vLLM only caches active pages, resulting in a memory usage of $P \times L \times d$, rather than the previous $N \times L \times d$.</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="math"/><category term="code"/><summary type="html"><![CDATA[Introduction to Machine Learning]]></summary></entry><entry><title type="html">Black-Scholes Model</title><link href="https://zane-liao.netlify.app//blog/2025/bs/" rel="alternate" type="text/html" title="Black-Scholes Model"/><published>2025-09-15T14:13:06+00:00</published><updated>2025-09-15T14:13:06+00:00</updated><id>https://zane-liao.netlify.app//blog/2025/bs</id><content type="html" xml:base="https://zane-liao.netlify.app//blog/2025/bs/"><![CDATA[<ul> <li> <p>The Black-Scholes model tells us that if there are no arbitrage opportunities in the market and stock prices exhibit certain random fluctuations, we can calculate the ‚Äòfair value‚Äô of a European option.</p> </li> <li> <p>European options are the most common type of option, where the two parties agree on an expiration date before settling profits or losses.</p> </li> <li> <p>American options are more flexible and therefore usually more expensive than European options, especially when stock prices are highly volatile or there are dividends, making early exercise potentially more worthwhile.</p> </li> <li> <p>The Black-Scholes model only applies to European options and can use a closed-form solution. It does not apply to American options, requiring other methods for calculation.</p> </li> <li> <p>This model allows us to mathematically convert the randomness of future stock price movements into the fair value of options today (more intuitive).</p> </li> <li> <p>Black-Scholes call and put options are derived from put-call parity:</p> </li> </ul> \[C - P = S - K e^{-r(T-t)}\] <ul> <li>Put Option</li> </ul> \[P = K e^{-r(T-t)} \Phi(-d_2) - S \Phi(-d_1)\] <ul> <li>Call option</li> </ul> \[C = S \Phi(d_1) - K e^{-r(T-t)} \Phi(d_2)\] <ul> <li>where \(d_1 = \frac{\ln(S/K) + (r + 0.5\sigma^2)(T-t)}{\sigma\sqrt{T-t}}, \quad d_2 = d_1 - \sigma \sqrt{T-t}\)</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import packages
</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># Define call option function
</span>
<span class="k">def</span> <span class="nf">bs_call</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>

<span class="n">d1</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">S</span><span class="o">/</span><span class="n">K</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">T</span><span class="p">))</span> 
<span class="n">d2</span> <span class="o">=</span> <span class="n">d1</span> <span class="o">-</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> 
<span class="k">return</span> <span class="n">S</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="n">d1</span><span class="p">)</span> <span class="o">-</span> <span class="n">K</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">r</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="n">d2</span><span class="p">)</span>

<span class="c1"># Define put option function
</span><span class="k">def</span> <span class="nf">bs_put</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span> 
<span class="n">d1</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">S</span><span class="o">/</span><span class="n">K</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">T</span><span class="p">))</span> 
<span class="n">d2</span> <span class="o">=</span> <span class="n">d1</span> <span class="o">-</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> 
<span class="k">return</span> <span class="n">K</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">r</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="o">-</span><span class="n">d2</span><span class="p">)</span> <span class="o">-</span> <span class="n">S</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="o">-</span><span class="n">d1</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Draw Black-Scholes Call Option Price
</span><span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">K</span><span class="o">=</span><span class="mi">100</span>
<span class="n">T</span><span class="o">=</span><span class="mi">1</span>
<span class="n">r</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">sigma</span><span class="o">=</span><span class="mf">0.2</span>

<span class="n">prices</span> <span class="o">=</span> <span class="p">[</span><span class="nf">bs_call</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">S</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">prices</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Stock Price S</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Call Option Price C</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Black-Scholes Call Option Price</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Draw Black-Scholes Call Option Price
</span><span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">K</span><span class="o">=</span><span class="mi">100</span>
<span class="n">T</span><span class="o">=</span><span class="mi">1</span>
<span class="n">r</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">sigma</span><span class="o">=</span><span class="mf">0.2</span>

<span class="n">prices</span> <span class="o">=</span> <span class="p">[</span><span class="nf">bs_put</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">S</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">prices</span><span class="p">,</span> <span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Stock Price S</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Put Option Price C</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Black-Scholes Put Option Price</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>


</code></pre></div></div>]]></content><author><name></name></author><category term="sample-posts"/><category term="math"/><category term="code"/><summary type="html"><![CDATA[Introduction to Black-Scholes Model in Factor Investing]]></summary></entry><entry><title type="html">Debug tricks</title><link href="https://zane-liao.netlify.app//blog/2025/debug/" rel="alternate" type="text/html" title="Debug tricks"/><published>2025-08-14T12:10:00+00:00</published><updated>2025-08-14T12:10:00+00:00</updated><id>https://zane-liao.netlify.app//blog/2025/debug</id><content type="html" xml:base="https://zane-liao.netlify.app//blog/2025/debug/"><![CDATA[<p><strong>These are basic commands and techniques I learned while conducting some teaching operating system experiments, mainly focusing on debugging.</strong></p> <p><strong>xv6</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c"># b9e416502121 =&gt; docker ps</span>
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
b9e416502121 ubuntu23-cs144 <span class="s2">"/bin/bash"</span> 13 hours ago Up 13 hours hardcore_wescoff
c1b373cdc67a ubuntu23-cs144 <span class="s2">"/bin/bash"</span> 27 hours ago Up 17 hours reverent_maxwell


<span class="c"># Start Kernel</span>
make qemu-gdb

gdb-multiarch kernel/kernel

<span class="nb">set </span>architecture riscv:rv64

target remote localhost:25000

<span class="c"># split-screen</span>
layout <span class="nb">split</span>

</code></pre></div></div> <p><strong>Pintos</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Start Kernel</span>
pintos <span class="nt">--qemu</span> <span class="nt">--gdb</span> <span class="nt">--run</span> user_program_name
<span class="nt">----</span>
<span class="c"># Find the function you want to Debug, for example</span>
<span class="nb">grep</span> <span class="nt">-rn</span> <span class="s2">"timer_sleep"</span>
...tests/threads/alarm-simultaneous.c:90: timer_sleep <span class="o">(</span>sleep_until - timer_ticks <span class="o">())</span><span class="p">;</span>
tests/threads/mlfqs-fair.c:101: timer_sleep <span class="o">(</span>40 <span class="k">*</span> TIMER_FREQ<span class="o">)</span><span class="p">;</span>
tests/threads/mlfqs-fair.c:116: timer_sleep <span class="o">(</span>sleep_time - timer_elapsed <span class="o">(</span>ti-&gt;start_time<span class="o">))</span><span class="p">;</span>
<span class="nb">grep</span>: threads/build/tests/threads/alarm-zero.o: binary file matches
<span class="nb">grep</span>: threads/build/tests/threads/alarm-simultaneous.o: binary file matches
<span class="nb">grep</span>: threads/build/tests/threads/mlfqs-fair.o: binary file matches
<span class="c"># We choose one arbitrarily, for example, alarm-zero</span>

pintos <span class="nt">--qemu</span> <span class="nt">--gdb</span> <span class="nt">--</span> run alarm-zero

<span class="nt">----</span> <span class="nb">cd </span>build

<span class="c"># Enter gdb</span>

pintos-gdb kernel.o

<span class="o">(</span>gdb<span class="o">)</span><span class="nb">set </span>architecture i386

<span class="o">(</span>gdb<span class="o">)</span>target remote localhost:1234

<span class="c"># For Example</span>

<span class="o">(</span>gdb<span class="o">)</span><span class="nb">break </span>devices/timer.c:96

<span class="o">(</span>gdb<span class="o">)</span>c

<span class="o">(</span>gdb<span class="o">)</span>step...

<span class="c"># If qemu freezes, execute this to find the PID</span>

ps aux | <span class="nb">grep </span>qemu

root 32 0.0 0.0 288508 5640 pts/1 S+ 03:51 0:00 make qemu-gdb

root 53 0.5 2.4 2016128 194744 pts/1 Sl+ 03:51 0:07 qemu-system-riscv64 <span class="nt">-machine</span> virt <span class="nt">-bios</span> none <span class="nt">-kernel</span> kernel/kernel <span class="nt">-m</span> 128M <span class="nt">-smp</span> 3 <span class="nt">-nographic</span> <span class="nt">-global</span> virtio-mmio.force-legacy<span class="o">=</span><span class="nb">false</span> <span class="nt">-drive</span> <span class="nv">file</span><span class="o">=</span>fs.img,if<span class="o">=</span>none,format<span class="o">=</span>raw,id<span class="o">=</span>x0 <span class="nt">-device</span> virtio-blk-device,drive<span class="o">=</span>x0,bus<span class="o">=</span>virtio-mmio-bus.0 <span class="nt">-S</span> <span class="nt">-gdb</span> tcp::25000

root 84 60.0 0.0 288516 4704 pts/2 S+ 04:14 0:00 <span class="nb">grep</span> <span class="nt">--color</span><span class="o">=</span>auto qemu

<span class="c"># kill process</span>

<span class="nb">kill</span> <span class="nt">-9</span> 32 53

</code></pre></div></div> <p><strong>Start qemu</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pintos <span class="nt">--qemu</span> <span class="nt">--run</span> alarm-multiple
</code></pre></div></div> <p><strong>GDB with Pintos</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c"># Set breakpoint</span>

<span class="nb">break </span>filename:line

<span class="c"># For Example</span>

<span class="nb">break </span>devices/timer.c:92

<span class="c"># Continue running the program until the next breakpoint</span>

<span class="k">continue</span> | c

<span class="c"># Step through the code, entering the function</span>

step | s

<span class="c"># Step through the code, but do not enter the function</span>

next | n

<span class="c"># Execute the current function and return to the calling function</span>

finish

<span class="c"># Run to the specified line number</span>

<span class="k">until</span> &lt;line&gt;

<span class="c"># Jump directly to the specified line number</span>

jump &lt;line&gt;

<span class="c"># List all breakpoints</span>

info breakpoints

<span class="c"># Display the contents of all registers</span>

info registers

<span class="c"># Display the current stack</span>

info frame

<span class="c"># Display all threads</span>

info threads

<span class="c"># Display all variables in the current function</span>

info locals

<span class="c"># Delete a breakpoint</span>

delete &lt;breakpoint_number&gt;

<span class="c"># Print the variable value</span>

print &lt;variable&gt;

<span class="c"># Display the current program code</span>

list | l

<span class="c"># Display the current function call stack</span>

backtrace | bt

<span class="c"># Quit</span>

</code></pre></div></div> <p><strong>Count rows</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#ubuntu/debian</span>
apt <span class="nb">install </span>diffstat

<span class="c">#Macos</span>
brew <span class="nb">install </span>diffstat

<span class="c"># For Example</span>
git diff | diffstat

<span class="c">#Commit</span>
git show &lt;commit_id&gt; | diffstat
<span class="c"># Or</span>
git diff &lt;old_commit&gt; &lt;new_commit&gt; | diffstat

<span class="c"># Git diff</span>
git diff <span class="nt">--stat</span>

</code></pre></div></div>]]></content><author><name></name></author><category term="sample-posts"/><category term="code"/><summary type="html"><![CDATA[Introduction to Debug with Operating System]]></summary></entry><entry><title type="html">Linear Factor Model</title><link href="https://zane-liao.netlify.app//blog/2025/quant/" rel="alternate" type="text/html" title="Linear Factor Model"/><published>2025-06-29T13:10:00+00:00</published><updated>2025-06-29T13:10:00+00:00</updated><id>https://zane-liao.netlify.app//blog/2025/quant</id><content type="html" xml:base="https://zane-liao.netlify.app//blog/2025/quant/"><![CDATA[<p><strong>CAPM asset pricing option formula\(E[R_i] - R_f= \beta (E[R_M] - R_f) \tag{1.1}\)</strong></p> <ul> <li>$R_i$ is the rate of return of an asset $i$</li> <li>$R_f$ is the risk-free rate of return</li> <li>$R_M$ is the rate of return of the market portfolio</li> <li>$\beta_i = \frac{\text{Cov}(R_i, R_m)}{\text{Var}(R_m)}$ the degree of exposure of asset $i$ to market risk</li> </ul> <p><strong>APT arbitrage pricing theory\(E[R^{e}_i] = \beta^{'}_i \lambda \tag{1.2}\) Pricing Error\(E[R^{e}_i] = \alpha_i + \beta^{'}_i \lambda \tag{1.3}\)</strong></p> <ul> <li>The expected excess return of an asset is the weighted average of its exposure to systematic risk factors ($\beta_i$‚Äã) multiplied by the premium of each factor ($\lambda$)</li> </ul> <p><strong>Factors are to assets as food is to nutrients, and we define the two necessary conditions that factors must meet as follows:</strong></p> <ul> <li>Factors drive the co-movement of asset returns, so factors must be related to the covariance matrix of asset returns</li> <li>In the long run, factors can obtain positive returns, which means that factors must be priced.</li> </ul> <p><strong>Pricing factors and anomaly factors</strong></p> <ul> <li>The academic community has rigorously divided the general types of factors, but the industry does not. The industry believes that factors that can obtain excess returns are good factors, and does not pay attention to the type of factors themselves</li> <li>We need to know that $\beta^{‚Äò}_i \lambda$ is a pricing model, so the factors it contains are called pricing factors, and $\alpha_i$ represents assets that can obtain multi-factor models but cannot explain excess returns, which are called anomaly factors</li> </ul> <p><strong>Multi-factor models are models of the mean. From the perspective of time series, asset excess returns and factor returns satisfy the following in time series:</strong> \(R^{e}_{it} = \alpha_i + \mathbf{\beta^{'}_i} \lambda_t + \epsilon_{it} \tag{1.4}\)</p> <ul> <li>$R^{e}_{it}$ represents the excess return of asset $i$ at time $t$</li> <li>$\lambda_t$ represents the factor return at time t</li> <li>$\epsilon_{it}$ represents the random disturbance at time t</li> </ul> <p><strong>We put the time series multivariate regression model of N assets together:</strong> \(R^{e}_{t} = \mathbf{\alpha} + \mathbf{\beta^{'}} \lambda_t + \mathbf{\epsilon_{t}} \tag{1.5}\)</p> <ul> <li>$R^{e}<em>t = [R^{e}</em>{1t}, R^{e}<em>{2t}, ‚Ä¶,R^{e}</em>{Nt}]$ Excess Return (With N-Dimension Vector)</li> <li>$\mathbf{\alpha} = [\alpha_1, \alpha_2, ‚Ä¶, \alpha_N]$ Pricing (With N-Dimension Vector)</li> <li>$\beta = [\beta_1, \beta_2, ‚Ä¶, \beta_N]\ is\ the\ N \times K$ factor exposure matrix (Matrix)</li> <li>$\mathbf{\epsilon_t} = [\epsilon_{1t}, \epsilon_{2t}, ‚Ä¶, \epsilon_{Nt}]$ random perturbation (With N-Dimension Vector)</li> </ul> <p><strong>The above formula satisfies $E[\epsilon_t] = 0$ and $cov(\lambda_t, \epsilon_t) = 0$. We calculate the Covariance Matrix ($\Sigma$) on both sides and get:</strong> \(\Sigma = \beta\Sigma_\lambda\beta^{'} + \Sigma_\epsilon \tag{1.6}\)</p> <ul> <li>$\Sigma$ is the Covariance Matrix of N assets (With N-Order Matrix)</li> <li>$\Sigma_\lambda$ is the Covariance Matrix of K factors (With K-Order Matrix)</li> <li>$\Sigma_\epsilon$ is the Covariance Matrix of N random perturbations (With N-Order Diagonal Matrix, Because $\epsilon_{it}$ are independent of each other)</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="math"/><category term="code"/><summary type="html"><![CDATA[Introduction to Linear Factor Models in Factor Investing]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://zane-liao.netlify.app//blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://zane-liao.netlify.app//blog/2024/photo-gallery</id><content type="html" xml:base="https://zane-liao.netlify.app//blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://zane-liao.netlify.app//blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://zane-liao.netlify.app//blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://zane-liao.netlify.app//blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We‚Äôre introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we‚Äôre introducing Gemini 1.5 Flash: a model that‚Äôs lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We‚Äôre also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5‚Äôs 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It‚Äôs optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it‚Äôs a lighter weight model than 1.5 Pro, it‚Äôs highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it‚Äôs been trained by 1.5 Pro through a process called ‚Äúdistillation,‚Äù where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash‚Äôs availability and pricing.Over the last few months, we‚Äôve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we‚Äôve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We‚Äôve improved control over the model‚Äôs responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we‚Äôve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we‚Äôre now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do ‚Äî not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we‚Äôre also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We‚Äôre announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we‚Äôve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind‚Äôs mission to build AI responsibly to benefit humanity, we‚Äôve always wanted to develop universal AI agents that can be helpful in everyday life. That‚Äôs why today, we‚Äôre sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do ‚Äî and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we‚Äôve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we‚Äôve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we‚Äôve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they‚Äôre being used in, and respond quickly, in conversation.With technology like this, it‚Äôs easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We‚Äôve made incredible progress so far with our family of Gemini models, and we‚Äôre always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we‚Äôre able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google‚Äôs privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let‚Äôs stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We‚Äôre sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://zane-liao.netlify.app//blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://zane-liao.netlify.app//blog/2024/tabs</id><content type="html" xml:base="https://zane-liao.netlify.app//blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="17416775-93b1-4829-9db5-b7a4b9122e14" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="17416775-93b1-4829-9db5-b7a4b9122e14" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="9e612a20-b3d2-489a-8afe-7035b882adda" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="9e612a20-b3d2-489a-8afe-7035b882adda" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="734d4e31-9ffb-44bd-a8fb-30ba1ffdc9a7" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="734d4e31-9ffb-44bd-a8fb-30ba1ffdc9a7" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://zane-liao.netlify.app//blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://zane-liao.netlify.app//blog/2024/typograms</id><content type="html" xml:base="https://zane-liao.netlify.app//blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry></feed>