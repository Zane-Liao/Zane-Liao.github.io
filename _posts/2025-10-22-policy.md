---
layout: post
title: Policy Gradient
date: 2025-10-22 19:20:00
description: Policy Gradient Derivation Process
tags: math code
categories: sample-posts
chart:
  # plotly: true
---

- **Derivation of Policy Gradient**

- The goal of reinforcement learning is to maximize reward $r$. The policy $\pi$ is defined as selecting the optimal action to maximize $r$.

- Policy Gradient is a method that directly adjusts the Action Method (Policy) to maximize reward.

- In Policy Gradient, our goal is to optimize the policy $$\pi(a|s,\theta)$$ to perform actions with higher rewards.

**We aim to maximize an objective function $$J(\theta)$$, which is typically the expected cumulative reward of the agent after executing policy $$\pi$$.**

$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^\infty \gamma^t r_t \right]$$

**Our Policy Gradient theorem is**

$$\nabla J(\theta) \propto \sum_{s} \mu(s) \sum_{a} q_\pi(s,a)\, \nabla_\theta \pi(a|s,\theta)$$

- $\nabla J(\theta)$ is the gradient of policy performance with respect to parameters.

- $\propto$ is proportional to, in simple terms, it's either directly proportional or inversely proportional; variable x increases or decreases along with x.

- $\sum_{s} \mu(s)$ is the robust distribution (state visitation frequency) of state $s$ under policy $\pi$, representing "how frequently the agent visits state $s" in the long run. Sometimes it's $\rho^\pi(s)$, but it's the same; it's a **sampling probability** used to weight the states.

- $\sum_a q_\pi(s,a)\,\nabla_ \theta\pi(a|s,\theta)$ is a weighted average of all possible actions $a$ for each state $s$.

- $q_\pi(s,a)$ in state s **Expected reward (Q-value)** after performing action a.

- $\nabla_\theta \pi(a|s,\theta)$ is the gradient of the policy function with respect to the parameters, representing "how to fine-tune the parameters to increase the probability of the action".

**Proof of our Policy Gradient Theorem (episodic case, from Sutton textbook)**

- The state-value function can be represented in the form of an action-value function.

$$
\begin{align*}
\nabla v_{\pi}(s) &= \nabla \left[\sum_{a}\pi(a|s)q_{\pi}(s,a)\right],\ \text{for all s $\in$ S} \\
&= \sum_{a}\left[\nabla \pi(a|s)q_{\pi}(s,a)+\pi(a|s)\nabla q_{\pi}(s,a)\right] \text{(rule of calculus)} \\
&= \sum_{a}\left[\nabla \pi(a|s)q_{\pi}(s,a)+\pi(a|s)\nabla \sum_{s', r} q(s',r|s,a)(r+v_{\pi}(s'))\right] \\
&= \sum_{a}\left[\nabla \pi(a|s)q_{\pi}(s,a)+\pi(a|s) \sum_{s'} q(s'|s,a)\nabla v_{\pi}(s')\right] \\
&= \sum_{a}\left[\nabla \pi(a|s)q_{\pi}(s,a)+\pi(a|s) \sum_{s'} q(s'|s,a) \sum_{a'}\nabla \pi(a'|s')q_{\pi}(s',a')+\pi(a'|s') \sum_{s''} q(s''|s',a')\nabla v_{\pi}(s'') \right] \\
&= \sum_{x \in S} \sum^{\infty}_{k=0} Pr(s \to x,k,\pi) \sum_{a} \nabla_{\pi}(a|x)q_{\pi}(x,a) \\
\end{align*}
$$

- The probability that we transition from state s to state x.

$$
\begin{align*}
\nabla J(\theta) &= \nabla v_{\pi}(s_{0}) \\
&= \sum_{s} \left(\sum^{\infty}_{k=0} Pr(s_{0} \to s,k,\pi)\right) \sum_{a} \nabla_{\pi}(a|s)q_{\pi}(s,a) \\
&= \sum_{s} \eta(s) \sum_{a} \nabla_{\pi}(a|s)q_{\pi}(s,a) \\
&= \sum_{s'} \eta(s') \sum_{s} \frac{\eta(s)}{\sum_{s'}\eta(s')} \sum_{a} \nabla_{\pi}(a|s)q_{\pi}(s,a) \\
&= \sum_{s'} \eta(s') \sum_{s} \mu(s) \sum_{a} \nabla_{\pi}(a|s)q_{\pi}(s,a) \\
&\propto \sum_{s} \mu(s) \sum_{a} \nabla_{\pi}(a|s)q_{\pi}(s,a)
\ \text{End.} \\
\end{align*}
$$

**Policy gradient methods generally include**
- Policy gradient classification includes On-policy, Off-policy, and Iteration.
- On-policy must be sampled from the current policy $\pi_{\theta}$.
- Off-policy allows sampling from other policies $\mu$ to update the policy $\pi$.
- In $\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \mu}\left[\frac{\pi_\theta(a|s)}{\mu(a|s)} \nabla_\theta \log \pi_\theta(a|s) \, G_t \right]$, $\frac{\pi_\theta}{\mu}$ belongs to importance sampling. (IS)
- Importance Sampling is a general method in probability theory used to calculate the expectation of a target distribution $p$, but the samples come from another distribution $q$. In $\mathbb{E}_{x \sim p}[f(x)] = \mathbb{E}_{x \sim q}\left[\frac{p(x)}{q(x)} f(x)\right]$, $\rho(x) = \frac{p(x)}{q(x)}$ are the IS weights.
- Iteration refers to both the concept of "policy iteration" (evaluation + improvement) and the **mini-batch multi-round updates** in actual implementation.
- The most basic **REINFORCE** method uses Montto Carlo sampling estimation.
- **REINFORCE with Baseline** method introduces a **baseline** to reduce variance.
- Actor-Critic method: the policy function is used to select actions, and the value function is used to evaluate the quality of the actions.

- Proximal Policy Optimization (PPO) Algorithms

- Direct Preference Optimization (DPO) Algorithm

- Group Relative Policy Optimization (GRPO) Algorithm (DeepSeek R1), etc.

- **Eligibility Traces** are a temporal credit allocation mechanism (TD(λ), Actor-Critic(λ)), essentially making gradients propagate more smoothly over time. In Deep RL, we use GAE (Generalized Advantage Estimation) instead.

- When implementing these algorithms, we distinguish between **episodic** (with an end, like the end of a game) and **continuing** (without an end, infinite). The focus is on PPO, DPO, and GRPO.

##### REINFORCE

$$\nabla J(\theta) \propto \sum_{s} \mu(s) \sum_{a} q_\pi(s,a)\, \nabla_\theta \pi(a|s,\theta) = \mathbb{E} \left[\sum_a q_{\pi}(S_t, a)\nabla_{\pi}(a|S_t, \theta) \right] \tag{13.6}$$

- Gradient-Ascent Algorithm Update-step:
$$\theta_{t+1} := \theta_t + \alpha \sum_a \hat{q}(S_t,a,w)\nabla_{\pi}(a|S_t,\theta)$$
- Continuing from 13.6, we deduce sampling (sampling).

$$\begin{aligned}
J(\theta) &= \mathbb{E_{\pi}} \left[ \sum_a \pi(a|S_t, \theta) q_{\pi}(S_t, a)\frac{\nabla_{\pi}(a|S_t, \theta)}{\pi(a|S_t, \theta)} \right] \\
&= \mathbb{E_{\pi}} \left[ q_{\pi}(S_t, A_t)\frac{\nabla_{\pi}(A_t|S_t, \theta)}{\pi(A_t|S_t, \theta)} \right] \text{(replacing a by the sample)}\\
&= \mathbb{E_{\pi}} \left[G_t \frac{\nabla_{\pi}(A_t|S_t, \theta)}{\pi(A_t|S_t, \theta)} \right] \text{[$\mathbb{E}_{\pi}[G_t|S_t,A_t]=q_{\pi}(S_t,A_t)$]}
\end{aligned}$$

- REINFORCE Update:
$$\theta_{t+1} := \theta_t + \alpha G_t \frac{\nabla_{\pi}(A_t|S_t, \theta)}{\pi(A_t|S_t, \theta)}$$

##### REINFORCE with Baseline

$$\nabla J(\theta) \propto \sum_{s} \mu(s) \left(\sum_{a} q_\pi(s,a)-b(s)\right) \nabla_\theta \pi(a|s,\theta)$$
- REINFORCE with Baseline Update:
$$\theta_{t+1} := \theta_t + \alpha \big(G_t - b(S_t)\big) \frac{\nabla_{\pi}(A_t|S_t, \theta)}{\pi(A_t|S_t, \theta)}$$

##### Actor-Critic
- Let’s look directly at the derivation in sutton, Update:
$$\begin{aligned}
\theta_{t+1} &:= \theta_t + \alpha \left( G_{t:t+1} - \hat{v}(S_t, w) \right) \frac{\nabla_{\pi}(A_t|S_t, \theta)}{\pi(A_t|S_t, \theta)} \\
&= \theta_t + \alpha \left( G_{t:t+1} - \gamma \hat{v}(S_{t+1}, w) - \hat{v}(S_t, w) \right) \frac{\nabla_{\pi}(A_t|S_t, \theta)}{\pi(A_t|S_t, \theta)} \\
&= \theta_t + \alpha \delta \frac{\nabla_{\pi}(A_t|S_t, \theta)}{\pi(A_t|S_t, \theta)}
\end{aligned}$$

##### Proximal Policy Optimization (PPO)

- Since algorithms like PPO differ significantly from those described in papers, derivation is omitted; the focus is on implementation and details.

- Key parts.
- Policy ratio: $r_t(\theta)=\dfrac{\pi_\theta(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)}$
- PPO-Clip surrogate $L^{\text{CLIP}}(\theta)=\mathbb{E}_t\Big[\min\big(r_t(\theta)\hat A_t,\; \operatorname{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat A_t\big)\Big]$.
- GAE (Advantage Estimation) $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t),\quad \hat A_t = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}$.
- Merge(total loss) value + entropy $L(\theta) = -L^{\text{CLIP}}(\theta) + c_1 \, \mathbb{E}_t\big[(V_\theta(s_t)-R_t)^2\big] - c_2 \, \mathbb{E}_t\big[ S[\pi_\theta](s_t)\big]$.

****
Algorithm 1: Proximal Policy Optimization (clipped surrogate)
****


```c
Initialize θ ← θ0
for iteration = 1, 2, ... do
    1. Collect set of transitions D = { (s_t, a_t, r_t, done_t, logπ_old_t, V_old_t) }
       by running policy π_{θ} in the environment for T × N steps:
           for t = 0 .. T-1 (parallel over N envs):
               a_t ~ π_{θ}(·|s_t)
               record logπ_old_t ← log π_{θ}(a_t|s_t)
               execute a_t → observe reward r_t and next state s_{t+1}, done_t
               record V_old_t ← V_{θ}(s_t)   # critic's prediction at sample time
    2. Compute advantages ˆA_t and returns R_t via GAE:
       # δ_t = r_t + γ V(s_{t+1}) (1 - done_{t+1}) - V(s_t)
       # ˆA_t = δ_t + γ λ δ_{t+1} + γ^2 λ^2 δ_{t+2} + ...
       Compute ˆA_t for all t by reversed recursion (see GAE formula).
       Set R_t ← ˆA_t + V_old_t
       Optionally normalize advantages: ˆA_t ← (ˆA_t - mean)/std

    3. Let θ_old ← θ   # freeze old policy for ratio computation

    4. For epoch = 1 .. K do
           Shuffle indices of D
           Partition D into minibatches of size M
           For each minibatch B do
               - Evaluate new log probabilities and values:
                   logπ_new_t ← log π_{θ}(a_t|s_t)   for t ∈ B
                   V_new_t    ← V_{θ}(s_t)
                   S_t        ← entropy of π_{θ}(·|s_t)

               - Compute probability ratios:
                   r_t ← exp( logπ_new_t − logπ_old_t )

               - Clipped surrogate objective (paper eq.7):
                   L^{CLIP}_t(θ) = min( r_t * ˆA_t,  clip(r_t, 1−ε, 1+ε) * ˆA_t )

               - Policy loss (we minimize negative surrogate):
                   L_{policy} = − E_{t∈B}[ L^{CLIP}_t(θ) ]

               - Value loss (MSE), optional value clipping:
                   L_{value} = E_{t∈B}[ (V_new_t − R_t)^2 ]       # optionally 0.5 * ...
                   (implementation may use clipped value loss to limit V update)

               - Entropy bonus:
                   L_{entropy} = E_{t∈B}[ S_t ]

               - Total loss:
                   L = L_{policy} + c1 * L_{value} − c2 * L_{entropy}

               - Gradient step:
                   θ ← θ − α · AdamGrad(∇_θ L)
                   (optionally clip gradients by norm)

           end for each minibatch
           (optional) compute approx KL between π_{θ_old} and π_{θ}; if > target_kl then break early
    end for epoch

    5. (optional) logging: avg episodic return, approx_kl, clip fraction, explained variance, etc.

end for iteration

```